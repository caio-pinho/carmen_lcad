{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu110\n"
     ]
    }
   ],
   "source": [
    "#Importar bibliotecas\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurar vari√°veis\n",
    "dataset_input = \"bin/dataset_input_consolidado_todos_comandos_sem_linhas_erro_e_50_comandos.txt\"\n",
    "dataset_output = \"bin/dataset_output_consolidado_todos_comandos_sem_linhas_erro_e_50_comandos.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch: 0 MSE: 0.53689\n",
      "Epoch: 1 MSE: 0.15423\n",
      "Epoch: 2 MSE: 0.09940\n",
      "Epoch: 3 MSE: 0.06372\n",
      "Epoch: 4 MSE: 0.03932\n",
      "Epoch: 5 MSE: 0.02350\n",
      "Epoch: 6 MSE: 0.01537\n",
      "Epoch: 7 MSE: 0.01107\n",
      "Epoch: 8 MSE: 0.00876\n",
      "Epoch: 9 MSE: 0.00741\n",
      "Epoch: 10 MSE: 0.00654\n",
      "Epoch: 11 MSE: 0.00592\n",
      "Epoch: 12 MSE: 0.00538\n",
      "Epoch: 13 MSE: 0.00489\n",
      "Epoch: 14 MSE: 0.00440\n",
      "Epoch: 15 MSE: 0.00397\n",
      "Epoch: 16 MSE: 0.00355\n",
      "Epoch: 17 MSE: 0.00320\n",
      "Epoch: 18 MSE: 0.00293\n",
      "Epoch: 19 MSE: 0.00271\n",
      "Epoch: 20 MSE: 0.00254\n",
      "Epoch: 21 MSE: 0.00240\n",
      "Epoch: 22 MSE: 0.00229\n",
      "Epoch: 23 MSE: 0.00221\n",
      "Epoch: 24 MSE: 0.00214\n",
      "Epoch: 25 MSE: 0.00208\n",
      "Epoch: 26 MSE: 0.00204\n",
      "Epoch: 27 MSE: 0.00200\n",
      "Epoch: 28 MSE: 0.00197\n",
      "Epoch: 29 MSE: 0.00194\n",
      "Epoch: 30 MSE: 0.00192\n",
      "Epoch: 31 MSE: 0.00189\n",
      "Epoch: 32 MSE: 0.00187\n",
      "Epoch: 33 MSE: 0.00186\n",
      "Epoch: 34 MSE: 0.00184\n",
      "Epoch: 35 MSE: 0.00183\n",
      "Epoch: 36 MSE: 0.00182\n",
      "Epoch: 37 MSE: 0.00180\n",
      "Epoch: 38 MSE: 0.00179\n",
      "Epoch: 39 MSE: 0.00177\n",
      "Epoch: 40 MSE: 0.00176\n",
      "Epoch: 41 MSE: 0.00175\n",
      "Epoch: 42 MSE: 0.00174\n",
      "Epoch: 43 MSE: 0.00174\n",
      "Epoch: 44 MSE: 0.00172\n",
      "Epoch: 45 MSE: 0.00172\n",
      "Epoch: 46 MSE: 0.00171\n",
      "Epoch: 47 MSE: 0.00170\n",
      "Epoch: 48 MSE: 0.00169\n",
      "Epoch: 49 MSE: 0.00168\n",
      "Epoch: 50 MSE: 0.00167\n",
      "Epoch: 51 MSE: 0.00167\n",
      "Epoch: 52 MSE: 0.00166\n",
      "Epoch: 53 MSE: 0.00165\n",
      "Epoch: 54 MSE: 0.00165\n",
      "Epoch: 55 MSE: 0.00164\n",
      "Epoch: 56 MSE: 0.00164\n",
      "Epoch: 57 MSE: 0.00163\n",
      "Epoch: 58 MSE: 0.00162\n",
      "Epoch: 59 MSE: 0.00161\n",
      "Epoch: 60 MSE: 0.00160\n",
      "Epoch: 61 MSE: 0.00160\n",
      "Epoch: 62 MSE: 0.00159\n",
      "Epoch: 63 MSE: 0.00159\n",
      "Epoch: 64 MSE: 0.00158\n",
      "Epoch: 65 MSE: 0.00158\n",
      "Epoch: 66 MSE: 0.00157\n",
      "Epoch: 67 MSE: 0.00157\n",
      "Epoch: 68 MSE: 0.00156\n",
      "Epoch: 69 MSE: 0.00156\n",
      "Epoch: 70 MSE: 0.00155\n",
      "Epoch: 71 MSE: 0.00155\n",
      "Epoch: 72 MSE: 0.00154\n",
      "Epoch: 73 MSE: 0.00154\n",
      "Epoch: 74 MSE: 0.00153\n",
      "Epoch: 75 MSE: 0.00153\n",
      "Epoch: 76 MSE: 0.00152\n",
      "Epoch: 77 MSE: 0.00152\n",
      "Epoch: 78 MSE: 0.00151\n",
      "Epoch: 79 MSE: 0.00151\n",
      "Epoch: 80 MSE: 0.00150\n",
      "Epoch: 81 MSE: 0.00150\n",
      "Epoch: 82 MSE: 0.00149\n",
      "Epoch: 83 MSE: 0.00149\n",
      "Epoch: 84 MSE: 0.00148\n",
      "Epoch: 85 MSE: 0.00148\n",
      "Epoch: 86 MSE: 0.00147\n",
      "Epoch: 87 MSE: 0.00146\n",
      "Epoch: 88 MSE: 0.00146\n",
      "Epoch: 89 MSE: 0.00145\n",
      "Epoch: 90 MSE: 0.00144\n",
      "Epoch: 91 MSE: 0.00144\n",
      "Epoch: 92 MSE: 0.00143\n",
      "Epoch: 93 MSE: 0.00142\n",
      "Epoch: 94 MSE: 0.00141\n",
      "Epoch: 95 MSE: 0.00140\n",
      "Epoch: 96 MSE: 0.00140\n",
      "Epoch: 97 MSE: 0.00139\n",
      "Epoch: 98 MSE: 0.00138\n",
      "Epoch: 99 MSE: 0.00137\n",
      "MSE: 0.00137\n",
      "RMSE: 0.03696\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7757819.5   -363524.875       2.687       7.901 7757828.5   -363530.219\n",
      "        2.569       7.668       0.03 ]] -> [ 7.7133284   0.01539319  0.01961338  7.703379    0.00692324  0.02149175\n",
      "  7.6837683   0.01209517  0.02134292  7.6929193  -0.01559371  0.01943743\n",
      "  7.702143    0.00820185  0.02684052  7.7171226   0.02155375  0.01108611\n",
      "  7.7010727   0.02627596  0.02796042  7.7107925   0.00866225  0.02188314\n",
      "  7.7107077   0.0099097   0.0217712   7.716799   -0.02227759  0.02694469\n",
      "  7.734256    0.00848677  0.01090933  7.7112465   0.00353453  0.01829093\n",
      "  7.7178507   0.00627491  0.02144506  7.7453012   0.00847245  0.00626075\n",
      "  7.740513    0.0085679   0.03558734  7.7306943  -0.00733497  0.01535182\n",
      "  7.7732134  -0.01506821  0.01801204  7.7629886   0.02193388  0.02320998\n",
      "  7.770187   -0.0032876   0.02330318  7.751758    0.00819251  0.01771667\n",
      "  7.7565026   0.01444272  0.02978456  7.762593   -0.00474322  0.03297126\n",
      "  7.767766    0.00714928  0.02480593  7.764924    0.00400572  0.03512063\n",
      "  7.7778573  -0.00816646  0.02103334  7.7820687   0.00371187  0.03527064\n",
      "  7.773786   -0.00698806  0.00537264  7.7766085   0.02013388  0.01545058\n",
      "  7.799618   -0.00345792  0.01817642  7.805405   -0.01169924  0.02560919\n",
      "  7.806487    0.01630041  0.01981098  7.797964    0.0123666   0.01647884\n",
      "  7.819076   -0.01373924  0.03117971  7.802956   -0.00010061  0.00960259\n",
      "  7.831111    0.01160318  0.03255954  7.8074327   0.00732924  0.01873263\n",
      "  7.8360496  -0.01685691  0.02199976  7.8249664   0.00384413  0.02262796\n",
      "  7.856118    0.01240502  0.01572861  7.83519     0.01780658  0.01997178\n",
      "  7.832028    0.01893402  0.02936891  7.855077    0.00857804  0.03409045\n",
      "  7.857291    0.009599    0.0195821   7.854747    0.00598173  0.0227247\n",
      "  7.8751335   0.01277991  0.01533684  7.872616    0.0208469   0.03361238\n",
      "  7.8680663  -0.01453724  0.00827297  7.874689    0.00778049  0.02398339\n",
      "  7.873104    0.00019948  0.01789436  7.8874073   0.00696822  0.01527702]\n",
      "Sample 1\n",
      "[7757788.381 -363523.27        0.2         5.107 7757765.097 -363534.908\n",
      "       0.616       6.581      -0.016] -> [ 6.594796   -0.00661809  0.02180105  6.574548   -0.0112023   0.02246909\n",
      "  6.564745   -0.00622026  0.01833282  6.559226   -0.00956482  0.01954399\n",
      "  6.552344   -0.0053057   0.02478669  6.5511036  -0.00384928  0.02068854\n",
      "  6.5409575  -0.00403562  0.02298677  6.5345087  -0.01128004  0.02210454\n",
      "  6.535882   -0.00804084  0.02140519  6.52617    -0.0149498   0.02020519\n",
      "  6.524925   -0.01082323  0.0155358   6.513997   -0.00411084  0.01867659\n",
      "  6.5108514  -0.00587523  0.01775541  6.509279   -0.00615769  0.01642596\n",
      "  6.4974995  -0.00815514  0.01925519  6.496322   -0.01099729  0.01798149\n",
      "  6.497087   -0.00810543  0.01871636  6.4875693  -0.00386253  0.0233941\n",
      "  6.479098   -0.01244943  0.01747956  6.477876   -0.00773492  0.02083286\n",
      "  6.4727006  -0.00439473  0.01907146  6.464226   -0.00868222  0.02225351\n",
      "  6.462495   -0.00373814  0.02380722  6.4549365  -0.00858243  0.0242696\n",
      "  6.4461637  -0.01075354  0.02175893  6.4483066  -0.00946028  0.02072877\n",
      "  6.4353757  -0.01078912  0.02093232  6.434137   -0.00599728  0.01958112\n",
      "  6.427384   -0.0114844   0.01930854  6.429825   -0.01302059  0.01908653\n",
      "  6.422445   -0.00623873  0.01979288  6.413166   -0.00818127  0.01675987\n",
      "  6.411924   -0.01427901  0.02225845  6.4016266  -0.00784987  0.01654665\n",
      "  6.3965273  -0.00948972  0.02020111  6.3889246  -0.00900804  0.01881592\n",
      "  6.386363   -0.01211131  0.02289973  6.3791485  -0.01098217  0.02284922\n",
      "  6.3786535  -0.00570049  0.01559128  6.370684   -0.00239713  0.02010725\n",
      "  6.3635983  -0.00164592  0.01981929  6.3643713  -0.00517848  0.02311473\n",
      "  6.363124   -0.00402329  0.01822853  6.351737   -0.01069637  0.02160195\n",
      "  6.3482823  -0.00772457  0.01739619  6.344368   -0.00599355  0.0238396\n",
      "  6.3386636  -0.00813379  0.01916131  6.333193   -0.00515646  0.02200397\n",
      "  6.322994   -0.00753851  0.01812402  6.323598   -0.00072626  0.01666553] (expected [ 6.581 -0.016  0.02   6.575 -0.016  0.02   6.568 -0.016  0.02   6.562\n",
      " -0.016  0.02   6.555 -0.016  0.02   6.549 -0.016  0.02   6.542 -0.016\n",
      "  0.02   6.536 -0.016  0.02   6.529 -0.016  0.02   6.522 -0.016  0.02\n",
      "  6.516 -0.016  0.02   6.509 -0.016  0.02   6.503 -0.016  0.02   6.496\n",
      " -0.016  0.02   6.49  -0.016  0.02   6.483 -0.016  0.02   6.477 -0.016\n",
      "  0.02   6.47  -0.016  0.02   6.464 -0.016  0.02   6.457 -0.016  0.02\n",
      "  6.451 -0.016  0.02   6.444 -0.017  0.02   6.438 -0.017  0.02   6.431\n",
      " -0.017  0.02   6.425 -0.017  0.02   6.418 -0.017  0.02   6.412 -0.017\n",
      "  0.02   6.405 -0.017  0.02   6.399 -0.017  0.02   6.392 -0.017  0.02\n",
      "  6.386 -0.017  0.02   6.379 -0.017  0.02   6.373 -0.017  0.02   6.366\n",
      " -0.017  0.02   6.36  -0.017  0.02   6.353 -0.017  0.02   6.347 -0.017\n",
      "  0.02   6.34  -0.018  0.02   6.334 -0.018  0.02   6.327 -0.018  0.02\n",
      "  6.321 -0.018  0.02   6.314 -0.018  0.02   6.308 -0.018  0.02   6.301\n",
      " -0.018  0.02   6.295 -0.019  0.02   6.288 -0.019  0.02   6.282 -0.019\n",
      "  0.02   6.275 -0.019  0.02   6.269 -0.019  0.02   6.262 -0.019  0.02 ])\n",
      "Sample 2\n",
      "[7757243.088 -364095.065      -0.123       8.55  7757209.482 -364092.264\n",
      "      -0.055       8.474      -0.003] -> [8.496039   0.0076143  0.020352   8.461106   0.00491933 0.02121185\n",
      " 8.458497   0.00784453 0.01987903 8.459768   0.00352937 0.01963263\n",
      " 8.460327   0.00813521 0.02311186 8.464119   0.00941466 0.01887262\n",
      " 8.461567   0.00936398 0.02276689 8.462386   0.00515129 0.02116106\n",
      " 8.466571   0.00697763 0.02093185 8.465568   0.00041783 0.02141814\n",
      " 8.4694605  0.00546005 0.01648776 8.466098   0.00847033 0.01911993\n",
      " 8.468415   0.00789231 0.01948323 8.472638   0.00797708 0.01675534\n",
      " 8.469935   0.00699493 0.01992467 8.4723     0.00406773 0.01812937\n",
      " 8.4785     0.00456318 0.01927938 8.476562   0.0097979  0.021851\n",
      " 8.476233   0.00390799 0.01971521 8.478046   0.00727615 0.01961639\n",
      " 8.479176   0.00934974 0.02117241 8.478733   0.00546026 0.02267003\n",
      " 8.48163    0.0090987  0.02207174 8.481133   0.00644545 0.02348318\n",
      " 8.4808855  0.00401029 0.02091687 8.485686   0.00594073 0.02101321\n",
      " 8.482138   0.00406071 0.01998842 8.485208   0.00876465 0.01903896\n",
      " 8.486464   0.0041434  0.01942682 8.491532   0.00241022 0.02078383\n",
      " 8.491369   0.00855133 0.01984033 8.48972    0.00731761 0.01807576\n",
      " 8.49372    0.00147417 0.02245657 8.491088   0.00618334 0.01709758\n",
      " 8.49361    0.0066188  0.02156535 8.491941   0.00648548 0.0192585\n",
      " 8.495762   0.00203955 0.02139144 8.494967   0.00511108 0.02161904\n",
      " 8.500134   0.00860152 0.01784362 8.498285   0.01070111 0.01995134\n",
      " 8.498056   0.01115454 0.02136567 8.503363   0.00847051 0.02302664\n",
      " 8.506349   0.00908807 0.0190965  8.503894   0.0054286  0.02091944\n",
      " 8.506912   0.00759836 0.0179132  8.508202   0.00907665 0.02334011\n",
      " 8.50854    0.00382648 0.01797446 8.509635   0.00826448 0.02165115\n",
      " 8.5078535  0.00603555 0.01905931 8.512559   0.01008599 0.01775693] (expected [ 8.474 -0.003  0.02   8.475 -0.003  0.02   8.475 -0.003  0.02   8.476\n",
      " -0.003  0.02   8.476 -0.003  0.02   8.476 -0.003  0.02   8.477 -0.003\n",
      "  0.02   8.477 -0.003  0.02   8.477 -0.003  0.02   8.478 -0.003  0.02\n",
      "  8.478 -0.003  0.02   8.479 -0.003  0.02   8.479 -0.003  0.02   8.479\n",
      " -0.003  0.02   8.48  -0.003  0.02   8.48  -0.003  0.02   8.48  -0.003\n",
      "  0.02   8.481 -0.003  0.02   8.481 -0.003  0.02   8.482 -0.003  0.02\n",
      "  8.482 -0.003  0.02   8.482 -0.003  0.02   8.483 -0.003  0.02   8.483\n",
      " -0.003  0.02   8.483 -0.003  0.02   8.484 -0.003  0.02   8.484 -0.003\n",
      "  0.02   8.485 -0.003  0.02   8.485 -0.003  0.02   8.485 -0.003  0.02\n",
      "  8.486 -0.003  0.02   8.486 -0.003  0.02   8.486 -0.003  0.02   8.487\n",
      " -0.003  0.02   8.487 -0.003  0.02   8.488 -0.003  0.02   8.488 -0.003\n",
      "  0.02   8.488 -0.003  0.02   8.489 -0.003  0.02   8.489 -0.003  0.02\n",
      "  8.489 -0.003  0.02   8.49  -0.003  0.02   8.49  -0.003  0.02   8.491\n",
      " -0.003  0.02   8.491 -0.003  0.02   8.491 -0.003  0.02   8.492 -0.003\n",
      "  0.02   8.492 -0.003  0.02   8.492 -0.003  0.02   8.493 -0.003  0.02 ])\n",
      "Sample 3\n",
      "[7756532.733 -363783.142      -1.908       8.55  7756551.866 -363754.871\n",
      "      -2.387       8.693       0.027] -> [ 8.608262    0.01352542  0.01715171  8.567442    0.01343659  0.01853983\n",
      "  8.559758    0.01207205  0.02334599  8.562125    0.00025803  0.02000974\n",
      "  8.565549    0.00962443  0.02054778  8.567326    0.01361822  0.01335538\n",
      "  8.561518    0.01515397  0.0232206   8.564838    0.01397183  0.01934353\n",
      "  8.557612    0.01228148  0.02028632  8.561785    0.00090981  0.02498143\n",
      "  8.564684    0.01351168  0.01745541  8.556232    0.00613705  0.01970303\n",
      "  8.555585    0.00891167  0.02346398  8.5631275   0.01023788  0.01502644\n",
      "  8.563751    0.01176438  0.02609923  8.554071    0.00582666  0.01784941\n",
      "  8.566722   -0.00083367  0.01979843  8.562947    0.01390561  0.01909678\n",
      "  8.566656    0.00940473  0.02501168  8.552978    0.01153896  0.01675382\n",
      "  8.552922    0.01179699  0.02746046  8.555958    0.00549603  0.02590072\n",
      "  8.553528    0.00798809  0.01944532  8.551878    0.01027457  0.02462461\n",
      "  8.558156    0.00534546  0.0189908   8.552364    0.01087254  0.0256855\n",
      "  8.552296    0.00607719  0.01366484  8.548573    0.01574864  0.01716106\n",
      "  8.557699    0.0088443   0.01906714  8.552446    0.00552207  0.02519529\n",
      "  8.552504    0.01499483  0.02001087  8.549684    0.01483924  0.02027631\n",
      "  8.553912    0.00560488  0.02486245  8.548457    0.00829081  0.01660333\n",
      "  8.55891     0.01595122  0.02709141  8.548147    0.01367666  0.02003005\n",
      "  8.55678     0.00257087  0.01838349  8.551254    0.01365243  0.01931937\n",
      "  8.559874    0.01429138  0.02165939  8.550237    0.01457887  0.01967558\n",
      "  8.548273    0.01470506  0.02637914  8.552109    0.0124763   0.02543824\n",
      "  8.548001    0.01219001  0.02135605  8.549418    0.01565485  0.02031278\n",
      "  8.555275    0.01717944  0.01837073  8.5509205   0.01983314  0.02465714\n",
      "  8.547183    0.00185738  0.01330122  8.548216    0.01305264  0.02156344\n",
      "  8.549213    0.01053165  0.02049757  8.549237    0.00904702  0.01931184] (expected [8.693 0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02\n",
      " 8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02\n",
      " 8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02\n",
      " 8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02\n",
      " 8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02\n",
      " 8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02\n",
      " 8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02  8.55  0.026 0.02\n",
      " 8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02\n",
      " 8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02\n",
      " 8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.027 0.02\n",
      " 8.55  0.027 0.02  8.55  0.027 0.02  8.55  0.028 0.02  8.55  0.028 0.02\n",
      " 8.55  0.028 0.02  8.55  0.028 0.02  8.55  0.028 0.02  8.55  0.028 0.02\n",
      " 8.55  0.028 0.02  8.55  0.029 0.02 ])\n",
      "Sample 4\n",
      "[7757663.632 -363609.237      -2.497       8.55  7757688.462 -363590.973\n",
      "      -2.509       7.703      -0.001] -> [ 7.744135    0.00019972  0.0177564   7.7153      0.00083093  0.01880823\n",
      "  7.716346   -0.00049118  0.02255933  7.7218623  -0.00677294  0.02000889\n",
      "  7.7280765  -0.00188636  0.02007798  7.7327495  -0.00047661  0.01565802\n",
      "  7.734767   -0.00000826  0.02203721  7.7408943   0.00082853  0.01941446\n",
      "  7.740691   -0.00038176  0.02016797  7.747692   -0.00608552  0.02335064\n",
      "  7.7529097   0.00043871  0.01853754  7.7537117  -0.00417491  0.01982422\n",
      "  7.7574205  -0.00249543  0.02254858  7.764914   -0.00177239  0.01733517\n",
      "  7.7703786  -0.00078765  0.02252844  7.7693334  -0.00381405  0.01850929\n",
      "  7.779035   -0.00795547  0.01998021  7.781942   -0.00060621  0.01914953\n",
      "  7.788538   -0.00164418  0.02357557  7.7854776  -0.00099018  0.01755735\n",
      "  7.7897086  -0.00133153  0.02488673  7.7959046  -0.00434524  0.02342426\n",
      "  7.7985177  -0.00339637  0.01924851  7.8022923  -0.00164445  0.02220824\n",
      "  7.810103   -0.00431675  0.01919793  7.810535   -0.00125606  0.02249188\n",
      "  7.815794   -0.00392615  0.01709926  7.817756    0.00074615  0.01813035\n",
      "  7.826627   -0.00222607  0.01937577  7.8273115  -0.00403397  0.02359896\n",
      "  7.831933    0.00065682  0.01999859  7.8353977   0.00086863  0.02035868\n",
      "  7.841164   -0.00387345  0.02280466  7.8434615  -0.00278364  0.01818172\n",
      "  7.8528705   0.00173837  0.02422252  7.852242    0.00052287  0.02005985\n",
      "  7.860379   -0.00572824  0.01864068  7.8622265   0.00075923  0.01937301\n",
      "  7.8702297   0.00072298  0.0215741   7.870054    0.00058775  0.01971966\n",
      "  7.873673    0.00062979  0.02410343  7.879095   -0.00014487  0.02294283\n",
      "  7.8808146  -0.00038728  0.02100647  7.886608    0.00222568  0.02004603\n",
      "  7.8935847   0.00291105  0.01890076  7.895484    0.00417524  0.02244485\n",
      "  7.897993   -0.00610223  0.01586231  7.902796    0.00055307  0.02105536\n",
      "  7.9082336  -0.00078918  0.02053381  7.911783   -0.00213087  0.01966354] (expected [ 7.703 -0.001  0.02   7.707 -0.002  0.02   7.712 -0.002  0.02   7.716\n",
      " -0.002  0.02   7.72  -0.002  0.02   7.725 -0.002  0.02   7.729 -0.002\n",
      "  0.02   7.734 -0.002  0.02   7.738 -0.002  0.02   7.742 -0.002  0.02\n",
      "  7.747 -0.002  0.02   7.751 -0.002  0.02   7.755 -0.003  0.02   7.76\n",
      " -0.003  0.02   7.764 -0.003  0.02   7.769 -0.003  0.02   7.773 -0.003\n",
      "  0.02   7.777 -0.003  0.02   7.782 -0.003  0.02   7.786 -0.003  0.02\n",
      "  7.791 -0.003  0.02   7.795 -0.003  0.02   7.799 -0.003  0.02   7.804\n",
      " -0.003  0.02   7.808 -0.003  0.02   7.813 -0.003  0.02   7.817 -0.003\n",
      "  0.02   7.821 -0.003  0.02   7.826 -0.003  0.02   7.83  -0.003  0.02\n",
      "  7.835 -0.003  0.02   7.839 -0.003  0.02   7.843 -0.003  0.02   7.848\n",
      " -0.003  0.02   7.852 -0.003  0.02   7.857 -0.003  0.02   7.861 -0.003\n",
      "  0.02   7.865 -0.003  0.02   7.87  -0.003  0.02   7.874 -0.003  0.02\n",
      "  7.879 -0.003  0.02   7.883 -0.003  0.02   7.887 -0.003  0.02   7.892\n",
      " -0.003  0.02   7.896 -0.003  0.02   7.901 -0.003  0.02   7.905 -0.003\n",
      "  0.02   7.909 -0.003  0.02   7.914 -0.003  0.02   7.918 -0.003  0.02 ])\n",
      "Sample 5\n",
      "[7757281.572 -363526.457      -2.388       8.55  7757303.017 -363505.124\n",
      "      -2.36        7.724       0.006] -> [ 7.756213    0.00319259  0.01838946  7.731133    0.00175623  0.01957069\n",
      "  7.7281065   0.00210579  0.02203624  7.734033   -0.00796145  0.01986234\n",
      "  7.7403893   0.00042052  0.02166937  7.747086    0.00405478  0.01515854\n",
      "  7.7454176   0.00526556  0.02321709  7.751834    0.00209215  0.02006534\n",
      "  7.7523165   0.00166673  0.02054956  7.7584724  -0.009076    0.02380567\n",
      "  7.7660284   0.00181081  0.01689537  7.7621117  -0.00204285  0.01946205\n",
      "  7.766403   -0.00041234  0.02200641  7.7774706   0.000515    0.01520454\n",
      "  7.7803245   0.00111887  0.02470281  7.778194   -0.00430276  0.01788219\n",
      "  7.7938538  -0.00861064  0.01953466  7.79397     0.0040513   0.02020185\n",
      "  7.800121   -0.00204174  0.0230998   7.7948995   0.0009279   0.01781574\n",
      "  7.7990837   0.00208084  0.02539194  7.804733   -0.00405025  0.02512347\n",
      "  7.8080473  -0.00076792  0.02059659  7.8103113  -0.0004098   0.02476086\n",
      "  7.8184023  -0.00482854  0.01972704  7.820205   -0.00024683  0.02473087\n",
      "  7.822168   -0.00432882  0.01520789  7.8246      0.00454297  0.01773046\n",
      "  7.8354893  -0.00246479  0.01915704  7.8377943  -0.00546357  0.02366202\n",
      "  7.841427    0.00373552  0.01994674  7.8422832   0.00301439  0.01938659\n",
      "  7.850925   -0.00583264  0.02432923  7.8494306  -0.00200988  0.01646291\n",
      "  7.8617973   0.00340849  0.02548995  7.8569293   0.00173856  0.01972559\n",
      "  7.868626   -0.00765693  0.01956813  7.867958    0.00110684  0.02022429\n",
      "  7.8801775   0.00305877  0.02007284  7.8761272   0.00420123  0.01979287\n",
      "  7.8782496   0.00449193  0.02478483  7.887149    0.00172076  0.025034\n",
      "  7.889289    0.00180891  0.02053482  7.8928204   0.0025795   0.02064373\n",
      "  7.902069    0.00456806  0.0181312   7.9032664   0.00712924  0.02462757\n",
      "  7.9044094  -0.00727858  0.01467832  7.909343    0.00205082  0.02166468\n",
      "  7.912876   -0.00054689  0.01987837  7.9187527   0.00021227  0.01863846] (expected [7.724 0.006 0.02  7.728 0.006 0.02  7.733 0.006 0.02  7.737 0.007 0.02\n",
      " 7.741 0.007 0.02  7.746 0.007 0.02  7.75  0.007 0.02  7.755 0.008 0.02\n",
      " 7.759 0.008 0.02  7.763 0.008 0.02  7.768 0.008 0.02  7.772 0.009 0.02\n",
      " 7.776 0.009 0.02  7.781 0.009 0.02  7.785 0.009 0.02  7.79  0.01  0.02\n",
      " 7.794 0.01  0.02  7.798 0.01  0.02  7.803 0.01  0.02  7.807 0.01  0.02\n",
      " 7.811 0.011 0.02  7.816 0.011 0.02  7.82  0.011 0.02  7.825 0.011 0.02\n",
      " 7.829 0.011 0.02  7.833 0.011 0.02  7.838 0.011 0.02  7.842 0.011 0.02\n",
      " 7.846 0.012 0.02  7.851 0.012 0.02  7.855 0.012 0.02  7.86  0.012 0.02\n",
      " 7.864 0.012 0.02  7.868 0.012 0.02  7.873 0.012 0.02  7.877 0.012 0.02\n",
      " 7.881 0.012 0.02  7.886 0.011 0.02  7.89  0.011 0.02  7.895 0.011 0.02\n",
      " 7.899 0.011 0.02  7.903 0.011 0.02  7.908 0.011 0.02  7.912 0.011 0.02\n",
      " 7.916 0.01  0.02  7.921 0.01  0.02  7.925 0.01  0.02  7.93  0.01  0.02\n",
      " 7.934 0.009 0.02  7.938 0.009 0.02 ])\n"
     ]
    }
   ],
   "source": [
    "#CONFIGURAR PARA VARIAS SAIDAS\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "#import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# Read data\n",
    "#data = fetch_california_housing()\n",
    "X = np.loadtxt(dataset_input,dtype='float',delimiter=\";\",usecols=np.arange(0,9))\n",
    "y = np.loadtxt(dataset_output,dtype='float',delimiter=\";\",usecols=np.arange(0,150))\n",
    "#X, y = data.data, data.target\n",
    " \n",
    "# train-test split for model evaluation\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    " \n",
    "\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    " \n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 150)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 150)\n",
    "\n",
    "# Define the model (clean)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 24),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(24, 12),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(12, 6),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(6, 150)\n",
    ")\n",
    "\"\"\"\n",
    "# Define the model (clean+1camada)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 12),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(12, 24),\n",
    "    torch.nn.Linear(24, 48),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(48, 24),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(24, 150)\n",
    ")\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 2400),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2400, 1200),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1200, 600),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(600, 150)\n",
    ")\n",
    "\n",
    "# Define the model - maior3\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 2400),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(2400, 1200),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1200, 600),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(600, 300),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(300, 150)\n",
    ")\n",
    "\n",
    "# Define the model - clean v2\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 240),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(240, 120),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(120, 60),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(60, 30),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(30, 150)\n",
    ")\n",
    "\n",
    "\n",
    "# Define the model - maior4\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 150),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(150, 300),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(300, 600),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(600, 1200),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1200, 150)\n",
    ")\n",
    "\"\"\"\n",
    "# loss function and optimizer\n",
    "loss_fn = torch.nn.MSELoss()  # mean square error\n",
    "#loss_fn = torch.nn.L1Loss()  # mean absolute error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)#inicial: 0.0001\n",
    " \n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 8  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "# Check if CUDA is available and set PyTorch to use GPU or CPU accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device) \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            end = min(start+batch_size, len(X_train))  # Add this line\n",
    "            X_batch = X_train[start:end].to(device)  # Modify this line\n",
    "            y_batch = y_train[start:end].to(device)  # Modify this line\n",
    "            #X_batch = X_train[start:start+batch_size]\n",
    "            #y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test.to(device))\n",
    "    mse = loss_fn(y_pred, y_test.to(device))\n",
    "    mse = float(mse)\n",
    "\n",
    "    print(\"Epoch: %d MSE: %.5f\" % (epoch,mse))\n",
    "\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    " \n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.5f\" % best_mse)\n",
    "print(\"RMSE: %.5f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    "\n",
    "from playsound import playsound\n",
    "playsound('/mnt/Dados/caiopinho/Downloads/bell-ringing-05.mp3') \n",
    " \n",
    "model.eval()\n",
    "\n",
    "values = [[7757819.500, -363524.875, 2.687, 7.901, 7757828.500, -363530.219, 2.569, 7.668, 0.030]]\n",
    "arr = np.array(values)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test out inference with specific example\n",
    "    for i in range(1):\n",
    "        X_sample = arr[i:i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32).to(device)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{arr} -> {y_pred[0].cpu().numpy()}\") \n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    np.set_printoptions(suppress=True)\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        print(f\"Sample {i+1}\")\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32).to(device)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].cpu().numpy()} (expected {y_test[i].cpu().numpy()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 1)\n"
     ]
    }
   ],
   "source": [
    "print(range(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.75781950e+06 -3.63524875e+05  2.68700000e+00  7.90100000e+00\n",
      "   7.75782850e+06 -3.63530219e+05  2.56900000e+00  7.66800000e+00\n",
      "   3.00000000e-02]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[7757819.5   -363524.875       2.687       7.901 7757828.5   -363530.219\n",
      "        2.569       7.668       0.03 ]]\n",
      "[[1.4283144  1.29725648 1.3274247  0.15065714 1.44142445 1.2609903\n",
      "  1.26205508 0.09564861 0.62923778]]\n",
      "[7757702.51  -363580.256      -2.52        8.55  7757726.475 -363561.254\n",
      "      -2.471       7.703       0.001] -> [7.7828856  0.04079258 0.01823798 7.7407737  0.03577656 0.01941196\n",
      " 7.743609   0.03876927 0.0198487  7.740826   0.04089189 0.01863489\n",
      " 7.7459316  0.04044801 0.01811176 7.742912   0.03889635 0.01826166\n",
      " 7.7489057  0.04273704 0.01807716 7.7529244  0.04113501 0.02040751\n",
      " 7.751785   0.03742373 0.0199624  7.7588606  0.04005572 0.02397382\n",
      " 7.750182   0.03713363 0.02273033 7.7601886  0.03957888 0.01747136\n",
      " 7.7515593  0.04269487 0.01997918 7.7602224  0.03432921 0.02263317\n",
      " 7.7690935  0.03806296 0.01977855 7.776273   0.04057875 0.02007185\n",
      " 7.767032   0.03684852 0.0198992  7.76936    0.03649157 0.02151653\n",
      " 7.7757545  0.03842759 0.02009675 7.7740073  0.03769806 0.01560675\n",
      " 7.77968    0.03532597 0.01700153 7.781092   0.03969678 0.01982882\n",
      " 7.783507   0.04164031 0.0198384  7.7851057  0.03866467 0.01999877\n",
      " 7.7787786  0.04324996 0.01820455 7.788921   0.04004541 0.01746831\n",
      " 7.7841606  0.03773472 0.01803799 7.7924013  0.03409918 0.01946514\n",
      " 7.7890334  0.04238528 0.01941639 7.802041   0.03861186 0.02017332\n",
      " 7.8036547  0.03676346 0.01681767 7.7991943  0.04018369 0.01817577\n",
      " 7.799094   0.03544647 0.02180261 7.7975063  0.03524449 0.021369\n",
      " 7.8110476  0.03621802 0.02014094 7.807754   0.03721243 0.02049983\n",
      " 7.8138785  0.03449732 0.02322033 7.811376   0.03608808 0.01959334\n",
      " 7.8191514  0.03210784 0.01834432 7.8181973  0.03676566 0.02071893\n",
      " 7.8050413  0.03796074 0.01949481 7.816806   0.03397202 0.02058761\n",
      " 7.8195887  0.03588006 0.0176142  7.8334985  0.03784168 0.0212826\n",
      " 7.818514   0.0336259  0.01935639 7.8305154  0.03393576 0.02058159\n",
      " 7.824135   0.02860087 0.02250949 7.8415785  0.03708565 0.01690523\n",
      " 7.8375497  0.03065245 0.02007796 7.827661   0.0387603  0.01953834] (expected [7.703 0.001 0.02  7.707 0.001 0.02  7.712 0.001 0.02  7.716 0.001 0.02\n",
      " 7.72  0.001 0.02  7.725 0.001 0.02  7.729 0.001 0.02  7.734 0.001 0.02\n",
      " 7.738 0.001 0.02  7.743 0.001 0.02  7.747 0.001 0.02  7.751 0.001 0.02\n",
      " 7.756 0.001 0.02  7.76  0.001 0.02  7.765 0.001 0.02  7.769 0.001 0.02\n",
      " 7.774 0.001 0.02  7.778 0.001 0.02  7.782 0.001 0.02  7.787 0.001 0.02\n",
      " 7.791 0.001 0.02  7.796 0.001 0.02  7.8   0.001 0.02  7.805 0.001 0.02\n",
      " 7.809 0.001 0.02  7.813 0.001 0.02  7.818 0.001 0.02  7.822 0.002 0.02\n",
      " 7.827 0.002 0.02  7.831 0.002 0.02  7.836 0.002 0.02  7.84  0.002 0.02\n",
      " 7.844 0.002 0.02  7.849 0.002 0.02  7.853 0.002 0.02  7.858 0.002 0.02\n",
      " 7.862 0.002 0.02  7.867 0.002 0.02  7.871 0.002 0.02  7.876 0.002 0.02\n",
      " 7.88  0.002 0.02  7.884 0.002 0.02  7.889 0.002 0.02  7.893 0.002 0.02\n",
      " 7.898 0.002 0.02  7.902 0.002 0.02  7.907 0.002 0.02  7.911 0.002 0.02\n",
      " 7.915 0.002 0.02  7.92  0.002 0.02 ])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "values = [[7757819.500, -363524.875, 2.687, 7.901, 7757828.500, -363530.219, 2.569, 7.668, 0.030]]\n",
    "arr = np.array(values)\n",
    "\n",
    "print(arr)\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(1):\n",
    "        X_sample = arr[i:i+1]\n",
    "        print(type(X_sample))\n",
    "        print(X_sample)\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        print(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32).to(device)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].cpu().numpy()} (expected {y_test[i].cpu().numpy()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7757857.5   -363554.156       2.437       8.55  7757865.5   -363561.062\n",
      "        2.411       0.          0.   ]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[7757857.5   -363554.156       2.437       8.55  7757865.5   -363561.062\n",
      "        2.411       0.          0.   ]]\n",
      "[[ 1.51083504  1.15395654  1.19824758  0.48101888  1.52198248  1.11059103\n",
      "   1.18156292 -4.04856651  0.05521917]]\n",
      "[7757241.302 -363563.136      -2.436       8.533 7757267.039 -363540.223\n",
      "      -2.394       8.685      -0.003] -> [-0.05147149 -0.00391084  0.02450544  0.02977848 -0.00676629  0.01925958\n",
      "  0.0564343  -0.00254185  0.02589587  0.07977614 -0.01059087  0.01908505\n",
      "  0.10933621 -0.00908232  0.01526111  0.1426277  -0.01116356  0.01913717\n",
      "  0.17357093 -0.01598471  0.02559212  0.19789107 -0.00453905  0.02584886\n",
      "  0.2222147  -0.00760896  0.02060728  0.25050014 -0.01071135  0.02303354\n",
      "  0.27755064 -0.01645463  0.02107035  0.31115305 -0.02493455  0.02105832\n",
      "  0.33694825 -0.01487887  0.02435225  0.35873532 -0.01567829  0.02066274\n",
      "  0.37271363 -0.01313848  0.02513582  0.41497153 -0.01503474  0.02144591\n",
      "  0.436026   -0.01540463  0.02103578  0.47361645 -0.02118599  0.02206871\n",
      "  0.4950617  -0.02481859  0.01271994  0.5278385  -0.01442153  0.0197677\n",
      "  0.5445486  -0.01889404  0.02215628  0.5707737  -0.02464486  0.02025475\n",
      "  0.6042311  -0.05134093  0.02377683  0.6371783  -0.03318469  0.01441504\n",
      "  0.6574966  -0.03040533  0.01153502  0.69206876 -0.0226476   0.01234078\n",
      "  0.71179676 -0.02728411  0.02199272  0.74453527 -0.03331544  0.02736633\n",
      "  0.7682854  -0.01491311  0.02066801  0.80121946 -0.0215913   0.01774708\n",
      "  0.8261597  -0.02814322  0.01736465  0.87526727 -0.0254825   0.02069124\n",
      "  0.88725674 -0.02571672  0.01490933  0.910434   -0.02240489  0.02481864\n",
      "  0.9422705  -0.01232403  0.02986433  0.9710986  -0.02803533  0.02133282\n",
      "  1.0050513  -0.01183711  0.01798139  1.0366316  -0.01590691  0.01835718\n",
      "  1.0609092  -0.0265411   0.01653484  1.0929476  -0.02244498  0.01922438\n",
      "  1.117502   -0.01988134  0.01632497  1.1475277  -0.02043995  0.02081062\n",
      "  1.1839249  -0.01915904  0.0132031   1.2103164  -0.01699897  0.02077518\n",
      "  1.2448758  -0.01566057  0.01872178  1.2638866  -0.00496652  0.01158461\n",
      "  1.2999274  -0.0127474   0.01704178  1.3212237  -0.00594763  0.02036707\n",
      "  1.3479265  -0.0239837   0.01958433  1.3847437  -0.02442656  0.01349927] (expected [ 8.685 -0.003  0.02   8.55  -0.003  0.02   8.55  -0.003  0.02   8.55\n",
      " -0.003  0.02   8.55  -0.003  0.02   8.55  -0.003  0.02   8.55  -0.003\n",
      "  0.02   8.55  -0.003  0.02   8.55  -0.003  0.02   8.55  -0.003  0.02\n",
      "  8.55  -0.003  0.02   8.55  -0.003  0.02   8.55  -0.003  0.02   8.55\n",
      " -0.003  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002\n",
      "  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02\n",
      "  8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55\n",
      " -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002\n",
      "  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02\n",
      "  8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55\n",
      " -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002\n",
      "  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02\n",
      "  8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55\n",
      " -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002\n",
      "  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02   8.55  -0.002  0.02 ])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "values = [[7757857.500, -363554.156, 2.437, 8.550, 7757865.500, -363561.062, 2.411, 0.000, 0.000]]\n",
    "arr = np.array(values)\n",
    "\n",
    "print(arr)\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(1):\n",
    "        X_sample = arr[i:i+1]\n",
    "        print(type(X_sample))\n",
    "        print(X_sample)\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        print(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[7757781.538 -363521.914      -2.797       7.479 7757813.153 -363521.892\n",
      "        2.827       8.2         0.051]]\n",
      "[[ 1.33226234  1.31622879 -1.39685001 -0.06478618  1.39973425  1.3065505\n",
      "   1.37944972  0.38450701  1.04319887]]\n",
      "[7757781.538 -363521.914      -2.797       7.479 7757813.153 -363521.892\n",
      "       2.827       8.2         0.051] -> [8.181326   0.05272616 0.02641746 8.140773   0.06074997 0.01603541\n",
      " 8.129248   0.05398777 0.01440225 8.135523   0.049911   0.01537436\n",
      " 8.1323805  0.05261231 0.02257932 8.1269865  0.04800221 0.01963068\n",
      " 8.124198   0.0509049  0.02371841 8.11923    0.05256447 0.01630135\n",
      " 8.114489   0.04791815 0.01409454 8.113762   0.05891797 0.02015051\n",
      " 8.111128   0.05003864 0.02361509 8.112732   0.05322156 0.01978908\n",
      " 8.113821   0.05088584 0.01970432 8.107655   0.05278888 0.02204894\n",
      " 8.105561   0.05155259 0.01740434 8.103662   0.04708258 0.01973448\n",
      " 8.100708   0.05222019 0.02272007 8.100998   0.05017047 0.01678706\n",
      " 8.091274   0.05167418 0.01685839 8.090018   0.04690822 0.0247036\n",
      " 8.096461   0.05405676 0.01682393 8.094748   0.050661   0.02127638\n",
      " 8.086491   0.05312075 0.01212277 8.089228   0.04447697 0.02096066\n",
      " 8.081609   0.0547631  0.01939246 8.077865   0.05345492 0.0201423\n",
      " 8.0810375  0.05437741 0.02708411 8.076733   0.05754933 0.02357301\n",
      " 8.076645   0.05426828 0.02095463 8.072516   0.05298759 0.02287421\n",
      " 8.0699415  0.05447268 0.0192939  8.064233   0.05423389 0.0195383\n",
      " 8.062398   0.05034141 0.01974893 8.059404   0.05339309 0.02247215\n",
      " 8.063102   0.05869516 0.02332221 8.054881   0.04618666 0.02758465\n",
      " 8.054841   0.05024753 0.02181757 8.058218   0.05386145 0.02372503\n",
      " 8.050544   0.05557213 0.02327129 8.046917   0.05065179 0.0218269\n",
      " 8.045702   0.05441321 0.01731804 8.042693   0.05150549 0.02021753\n",
      " 8.039089   0.05001142 0.02420444 8.040739   0.03592366 0.01753216\n",
      " 8.04229    0.04844355 0.02261725 8.036257   0.05217041 0.01469718\n",
      " 8.032951   0.05186881 0.02124875 8.023761   0.0499049  0.02194553\n",
      " 8.028658   0.04910132 0.02256993 8.017795   0.05136245 0.02107617] (expected [8.2   0.051 0.02  8.196 0.051 0.02  8.193 0.052 0.02  8.189 0.052 0.02\n",
      " 8.186 0.052 0.02  8.182 0.052 0.02  8.178 0.052 0.02  8.175 0.052 0.02\n",
      " 8.171 0.052 0.02  8.168 0.052 0.02  8.164 0.052 0.02  8.161 0.052 0.02\n",
      " 8.157 0.052 0.02  8.154 0.052 0.02  8.15  0.052 0.02  8.147 0.052 0.02\n",
      " 8.143 0.052 0.02  8.14  0.052 0.02  8.136 0.052 0.02  8.133 0.052 0.02\n",
      " 8.129 0.052 0.02  8.126 0.052 0.02  8.122 0.052 0.02  8.119 0.052 0.02\n",
      " 8.115 0.052 0.02  8.112 0.052 0.02  8.108 0.052 0.02  8.105 0.052 0.02\n",
      " 8.101 0.052 0.02  8.098 0.052 0.02  8.094 0.052 0.02  8.09  0.052 0.02\n",
      " 8.087 0.052 0.02  8.083 0.052 0.02  8.08  0.052 0.02  8.076 0.052 0.02\n",
      " 8.073 0.052 0.02  8.069 0.052 0.02  8.066 0.052 0.02  8.062 0.052 0.02\n",
      " 8.059 0.052 0.02  8.055 0.052 0.02  8.052 0.052 0.02  8.048 0.052 0.02\n",
      " 8.045 0.052 0.02  8.041 0.052 0.02  8.038 0.052 0.02  8.034 0.052 0.02\n",
      " 8.031 0.052 0.02  8.027 0.052 0.02 ])\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(1):\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        print(type(X_sample))\n",
    "        print(X_sample)\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        print(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7757473.312 -364063.127       0.536       8.55  7757443.388 -364078.829\n",
      "        0.415       8.441       0.01 ]]\n",
      "[[ 0.60346645 -1.34371932  0.25900089  0.48006168  0.52456725 -1.42475229\n",
      "   0.19457686  0.51666422  0.25139233]]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    dummy_input = X_test_raw[i: i+1]\n",
    "    print(dummy_input)\n",
    "    dummy_input = scaler.transform(dummy_input)\n",
    "    print(dummy_input)\n",
    "    dummy_input = torch.tensor(dummy_input, dtype=torch.float32)\n",
    "    traced_script_module = torch.jit.trace(model, dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module.save(\"model_clean_lr_0.001_200_epocas.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7757220.40727782 -363790.16679552       0.01492702       7.60444466\n",
      " 7757223.32321861 -363788.65378278       0.02261824       7.48120795\n",
      "      -0.00295132]\n",
      "[419.08663509 203.13781338   2.01185786   1.96965384 419.51681513\n",
      " 203.66713538   2.01659004   1.85767083   0.05151835]\n"
     ]
    }
   ],
   "source": [
    "mean = scaler.mean_\n",
    "std_dev = scaler.scale_\n",
    "print(mean)\n",
    "print(std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7757225.05264482 -363788.90212366       0.02647504       7.60650176\n",
      " 7757227.86445566 -363787.52767805       0.02886931       7.49126291\n",
      "      -0.00282856]\n",
      "[418.6078148  203.42717893   2.01170859   1.96145779 418.95064762\n",
      " 203.91455788   2.0160845    1.85034947   0.0512243 ]\n"
     ]
    }
   ],
   "source": [
    "#maior1\n",
    "mean = scaler.mean_\n",
    "std_dev = scaler.scale_\n",
    "print(mean)\n",
    "print(std_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann_clean.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann_clean.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann_clean.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_test_cpp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m7757481.724\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m363634.012\u001b[39m, \u001b[39m2.313\u001b[39m, \u001b[39m8.372\u001b[39m, \u001b[39m7757506.813\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m363650.764\u001b[39m, \u001b[39m2.799\u001b[39m, \u001b[39m7.703\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.037\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann_clean.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X_test_cpp \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(X_test_cpp)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann_clean.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(X_test_cpp)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    163\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:1003\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X, copy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    989\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform standardization by centering and scaling.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \n\u001b[1;32m    991\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[39m        Transformed array.\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1003\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1005\u001b[0m     copy \u001b[39m=\u001b[39m copy \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy\n\u001b[1;32m   1006\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m   1007\u001b[0m         X,\n\u001b[1;32m   1008\u001b[0m         reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         force_all_finite\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1013\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:1461\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not an estimator instance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator))\n\u001b[1;32m   1460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1461\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_test_cpp = np.array([7757481.724, -363634.012, 2.313, 8.372, 7757506.813, -363650.764, 2.799, 7.703, -0.037])\n",
    "X_test_cpp = scaler.transform(X_test_cpp)\n",
    "print(X_test_cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0573,  1.5971, -0.0614, -0.1519,  2.0956,  0.6641,  0.5896, -0.4267,\n",
      "          1.4114]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.randn(1, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=9, out_features=24, bias=True)\n",
       "  (1): Linear(in_features=24, out_features=12, bias=True)\n",
       "  (2): Linear(in_features=12, out_features=6, bias=True)\n",
       "  (3): Linear(in_features=6, out_features=150, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2144,  1.4291, -1.2174, -0.6024,  0.2654,  1.4626,  1.5191, -0.3682,\n",
      "          1.2702]])\n"
     ]
    }
   ],
   "source": [
    "# Assuming that you have a trained model 'model'\n",
    "#dummy_input = torch.randn(1, 9)  # Adjust as necessary\n",
    "dummy_input = X_test_raw[i: i+1]\n",
    "dummy_input = scaler.transform(dummy_input)\n",
    "dummy_input = torch.tensor(dummy_input, dtype=torch.float32)\n",
    "print(dummy_input)\n",
    "torch.onnx.export(model, dummy_input, \"model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/Dados/caiopinho/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.ones(1, 9, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_create_function_from_trace(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: str, arg1: function, arg2: tuple, arg3: function, arg4: bool, arg5: bool) -> torch._C.ScriptFunction\n\nInvoked with: '__torch__.onnx.onnx_ml_pb2.ModelProto', ir_version: 6\nopset_import {\n  version: 9\n}\nproducer_name: \"pytorch\"\nproducer_version: \"1.7\"\ngraph {\n  node {\n    input: \"input.1\"\n    input: \"0.weight\"\n    input: \"0.bias\"\n    output: \"9\"\n    name: \"Gemm_0\"\n    op_type: \"Gemm\"\n    attribute {\n      name: \"alpha\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"beta\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"transB\"\n      type: INT\n      i: 1\n    }\n  }\n  node {\n    input: \"9\"\n    input: \"1.weight\"\n    input: \"1.bias\"\n    output: \"10\"\n    name: \"Gemm_1\"\n    op_type: \"Gemm\"\n    attribute {\n      name: \"alpha\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"beta\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"transB\"\n      type: INT\n      i: 1\n    }\n  }\n  node {\n    input: \"10\"\n    input: \"2.weight\"\n    input: \"2.bias\"\n    output: \"11\"\n    name: \"Gemm_2\"\n    op_type: \"Gemm\"\n    attribute {\n      name: \"alpha\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"beta\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"transB\"\n      type: INT\n      i: 1\n    }\n  }\n  node {\n    input: \"11\"\n    input: \"3.weight\"\n    input: \"3.bias\"\n    output: \"12\"\n    name: \"Gemm_3\"\n    op_type: \"Gemm\"\n    attribute {\n      name: \"alpha\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"beta\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"transB\"\n      type: INT\n      i: 1\n    }\n  }\n  name: \"torch-jit-export\"\n  initializer {\n    dims: 24\n    data_type: 1\n    name: \"0.bias\"\n    raw_data: \"\\\"\\037\\361>3t\\362>\\203u\\311>,\\371\\342>[\\365}\\276\\323\\310\\227>\\344i\\350\\276\\243\\331\\272\\276F\\032x\\276\\236V\\373>\\306P\\362\\276&\\363 >[\\373\\267>\\216\\213\\324\\276\\204\\010\\222\\276\\024w\\353>y\\026\\223\\276y\\322\\n\\277*+\\t\\277Fnh>\\267\\250\\330\\276\\272\\361\\034?G\\002\\217>0w\\275\\276\"\n  }\n  initializer {\n    dims: 24\n    dims: 9\n    data_type: 1\n    name: \"0.weight\"\n    raw_data: \"\\356R\\027\\276\\213c1\\276\\017dU\\276\\3406U\\276\\366\\347\\307=X\\267C<\\370P\\013\\276$\\0138\\276\\351\\265\\\"\\276\\217\\236\\030>\\0361\\213=\\317\\3456\\276\\034\\022\\331=vM5\\275\\010\\346\\353=\\252\\237\\311<\\310\\0048\\273i1\\203>S>`\\276\\374`\\226\\276\\202\\261\\245=\\005\\027\\214\\276\\327\\300\\253\\275\\203~\\313\\275\\'0Q=\\337\\323w>\\232\\275@>/\\023\\202>\\357\\311u\\274M>]>$*C\\275\\2450\\223\\276z\\344J\\275+\\351\\205=\\204\\313\\004\\276\\366\\223\\371\\274\\322\\215h\\276\\212|\\223>\\365G\\301=\\271\\014g=\\205P\\036>t\\356\\272=\\221\\032\\344\\275\\205\\274\\210\\276V`\\353=\\312Z\\267\\275Y\\005\\217>4U\\360=\\236\\037\\321\\275[\\337\\231<\\307J\\245\\276\\375\\271\\232\\274o\\204$=\\323\\245\\204>\\\\\\305r\\276K\\213\\367=\\013\\247\\202\\276-\\377\\255\\276\\262\\366\\211=(^i>Eu]\\276@\\347O>\\036%\\260\\275\\033\\356Q>\\217\\201\\334\\275\\214\\237\\035>x\\030?>c\\025\\230>\\252]\\265;\\330,\\345\\275\\202\\263G\\276\\024\\202\\206><\\315Y\\2760\\372\\212>^\\264\\230\\276\\336\\343\\345:\\356\\214X\\276\\372\\230}>w\\006J\\276O\\035\\013>\\341\\220\\274;\\2622r\\276\\375M\\232>\\203S\\341=\\244\\260\\037\\276B\\360d>?N\\361=\\000FO>\\014)\\324>i!\\275\\275XM\\'>\\301\\374\\277\\274\\216\\022i>\\0241\\336\\274\\000\\353\\020>\\022%\\025>\\301\\352\\217\\274\\253\\200\\205\\276\\242\\243\\266><.\\274=\\310\\365<=Y\\2443\\276\\310\\017\\360=\\312\\357\\204\\276\\036S\\211>\\237;\\207=\\363Z\\313=\\303\\217\\355=\\371v\\220=e\\332\\203>\\321\\002l>Q]\\332\\273\\325\\227\\002\\276\\300\\3725=|R\\'>x\\277\\326\\275m\\036\\272\\275\\225\\325\\232\\275\\024\\234\\212\\276\\254\\t\\367;\\302\\351\\214\\276\\367\\200E\\276\\002t\\332=\\207\\363z\\276C7\\321\\276Ht\\000;b\\rg>\\225Q$=\\261\\315\\n\\276>\\004J\\276\\241\\315K>\\356F \\276\\207\\n\\323>\\005k\\023\\275\\356\\024t\\276RB\\277=\\037pp<\\004\\210A\\275\\300m\\200\\275\\021\\2229>>[d=\\374z\\323=\\021\\232\\226\\275\\023\\263\\220\\2759\\371\\221\\276\\353m\\202\\276HL?\\276F\\230s>~1\\035\\2769+\\000>\\336\\022\\223>\\010\\017\\330\\276\\004s-\\276\\235\\2522>.\\222\\355=\\220[\\257=\\362e\\313\\276\\274\\366\\314<)\\345\\354=\\255\\000\\231>]\\322(\\276\\205K\\233\\276\\205\\027\\253=\\224\\351\\312\\275:\\321D>\\254~Q\\275+\\321u\\276\\016\\377\\021\\276@\\352o\\275\\275\\330?>7)c>7H\\230\\275\\353\\346M\\276\\025\\222)=\\371\\315w\\276p\\327V\\276V \\r=j\\030B\\275g\\216\\232>:Z\\307=y\\314\\200\\275\\025\\274\\360\\275\\362-;\\275s\\303\\221\\276\\261\\224\\344\\275\\321Uu\\275\\274\\\"\\376<;\\021\\204\\276\\346i\\253\\273\\r\\275[=[\\026\\314\\275gm\\237\\273\\257\\317X>\\263\\374\\203>s\\005+>?\\240\\225\\276\\013\\312\\215>\\025-\\275=!\\331w\\275\\031C\\014=\\r\\372\\344\\275}\\374?\\276\\346\\240\\216\\276\\226\\361\\000=I\\310\\316\\276\\271-\\020>I\\251\\366\\275\\005\\361\\203\\276.\\243\\251>\\375\\027\\003\\274\\273\\237\\001>&\\246\\001\\274\\331\\327|\\274\\334\\362\\033\\276\\360\\234\\323\\276\\034\\317\\305\\275\"\n  }\n  initializer {\n    dims: 12\n    data_type: 1\n    name: \"1.bias\"\n    raw_data: \"\\t\\021\\325\\2765\\343 \\276\\201=\\323\\276E\\266\\237\\276\\230\\317\\263\\276\\002\\t\\322\\276\\245\\372$\\276pd\\230\\276\\307\\267\\257= \\342\\241\\276\\227\\363\\302\\276\\n\\321\\303\\276\"\n  }\n  initializer {\n    dims: 12\n    dims: 24\n    data_type: 1\n    name: \"1.weight\"\n    raw_data: \"\\250?\\037\\276\\020S\\225\\276!>?\\276\\313\\320\\016\\276\\345\\315\\234>\\331l\\013\\276\\277L&>tb\\201>2[\\031>RT\\312\\276\\323\\r\\034>\\220\\035\\002\\276\\315=\\217\\276\\254\\232\\227>\\241\\363$>%\\203\\265\\276\\262\\014\\221>\\214\\272\\367<\\3677\\366=KC\\020\\276t\\365\\235\\275\\246\\326+\\276\\022\\341\\251\\276\\037\\031\\323>\\373\\020\\177\\276\\312\\227Q\\276Ku\\363\\275\\371l\\255\\275\\r\\364y>R\\256>\\2766\\373\\317>\\014\\033\\311>\\203\\311\\241<\\027q\\031\\276z2\\247>\\367\\312\\346\\275\\377\\262\\324\\276\\334\\037\\302=\\035\\345\\025>\\322r|\\276\\353\\003\\220\\275\\243\\273\\264>\\254\\347\\002>\\335\\254\\214=|\\356\\253>mm\\265\\276\\245\\252j\\276#M_>yU\\252\\275\\334\\205d\\276\\036s\\225\\276\\255\\212\\314\\275\\254\\222\\014>\\256\\363\\323\\275\\346\\302$> \\372@>\\222BP>\\267\\205\\243\\276\\024\\002\\217=V F\\275\\230\\016\\031<.+\\370=^\\310\\276;\\034I\\320\\274\\201\\340\\250>\\222\\024\\254<\\\\\\t6>m\\250\\203\\276\\340B_>/\\277\\215\\276\\325\\311\\210\\276m\\226s=77\\031\\275\\023\\3351\\275\\026ia\\276\\304\\247\\221\\276\\222=\\353=\\241h\\216=9aB=\\335<;<{w\\321;\\032NQ\\276\\202\\352\\006=bU\\215\\2751\\211\\247\\276:* >n\\023 =\\233qt\\276\\211;\\231\\274\\212\\024\\220>\\343\\247R>_A|\\276\\t\\315\\024>D\\207\\253\\274\\321b\\224\\276\\022t\\266>\\372\\2539\\276\\263\\260{\\276\\315\\242\\207\\275f\\324\\034\\276A\\252\\225=12\\023\\275g\\351\\310>\\341(\\334=\\242tg\\275{\\016K\\276O^B>\\010H\\260\\275mc\\234\\276y\\373O>\\214\\3437>c\\374q\\276\\254\\370h>h\\352\\212=^\\310\\036=\\214\\315\\276\\275\\020R\\264>D\\236\\303\\275\\351\\304\\332\\275\\025\\277\\254>D\\241\\236\\276^r\\251\\276\\375o\\231<|j\\352\\275y\\271\\257\\275\\222\\031\\024\\276$\\t\\016=v\\271\\246>\\030\\205J\\273N\\017\\\\\\276:\\217\\217>#v\\032\\276\\257dB\\2765{\\206<T\\337\\364\\275\\022R=\\275\\275M*=\\263\\025\\214>!&\\207>\\317\\211\\200\\276\\304\\314\\236:\\206<\\\"\\276f\\362\\227\\276\\210cQ>B\\372\\'\\276L\\013d\\276(K\\364\\275\\361C\\372\\275\\234\\364\\367=\\273w\\243\\275\\303\\350S>\\207\\226\\234>\\276Z\\314=\\356l\\261\\275\\226%\\325>~\\020.>\\373\\260\\303\\276\\257\\273\\206>\\344\\223\\r=M\\247[\\276\\032\\237\\233>\\276@\\217<P\\212\\216>\\026\\267t\\275\\200:\\261>\\367\\311}\\276\\243\\322/\\275\\377 ?>\\274\\377+\\276|\\'\\205\\276\\372\\337c\\2756\\202\\370\\276\\302\\377~\\275;\\261]\\276\\024\\302\\326=!\\001\\036>\\274\\221S>*\\346\\256\\274\\\\=\\203>\\227\\021\\014=>\\217\\027\\275\\227\\361\\307>#\\305\\032>\\340v\\230\\276\\360\\250\\232\\275\\303b\\313>nv\\251>\\010tZ\\274\\223\\\\\\246=\\276\\314\\301\\2762\\247\\204=*\\376];\\343\\364m>\\327Bj>\\256Sn>\\370\\346\\256>\\3337\\340\\2753M\\364\\272\\232\\022\\346\\275J\\276\\276\\275\\252\\237\\006\\276\\255\\220\\353=\\013\\270\\017\\276\\035\\022S>\\274\\210Q>\\342>)\\276\\345OS\\276\\376\\007M=\\240\\212e<O\\323)\\276\\202(\\277\\276\\371\\261v>\\374!\\244\\276\\355}\\263>\\\\\\365\\022>\\240\\330D\\276\\216\\177+\\275\\257=\\005\\276\\260\\222\\333\\275\\353\\356\\232\\276I\\212L>\\271c\\255=&19><\\240;=\\330oh>\\3216_\\276A\\202\\262>\\037\\351s\\274\\2624\\231=X\\241\\367=\\367\\232\\233\\272\\256\\357\\004\\276\\037n/>\\020\\375\\252> \\273\\355=\\233\\275\\000\\275Q\\360m>o\\030/\\274K\\200\\271<\\376lp>\\2528\\325<:\\273\\271\\274k\\346|\\275\\231A\\261\\275\\324\\'6\\275\\014\\330?\\2743\\235\\250<\\246\\213\\265>\\035-Q>?\\364\\252\\276-\\257\\215>]CI\\275\\220\\005\\t\\276qv\\356<\\\\\\337d>\\262\\316D\\276\\177\\277\\205>\\035o\\203>\\342<\\271>{\\303\\212=!t;>\\300\\313E\\2766bH\\276\\035\\360\\017>\\346}*\\276\\361o\\246\\276l\\355k\\276v\\323\\252\\276\\024\\230\\304=\\263\\\"P\\276\\004\\370]>v\\357\\263<\\253v\\\"\\276\\315N\\332\\275\\372\\037\\211>\\017\\222\\340;A\\031>\\275\\265tt\\275\\237uI=\\355\\203,\\276\\261\\321\\232=\\202h\\237>\\266\\232\\322=\\261\\325u\\276\\332Wa>;\\003]\\276\\004\\216M=\\014\\253\\243>\"\n  }\n  initializer {\n    dims: 6\n    data_type: 1\n    name: \"2.bias\"\n    raw_data: \"\\240\\240\\353>\\251\\303\\313\\276N\\\"\\304>\\361\\216\\306\\2764\\331\\237>\\271\\331X\\276\"\n  }\n  initializer {\n    dims: 6\n    dims: 12\n    data_type: 1\n    name: \"2.weight\"\n    raw_data: \"+\\254\\230\\2763\\342\\245\\273n\\245\\345\\275E\\230Q\\276\\257\\262\\274\\276\\243\\000\\243\\276\\330qG\\276\\274%v=Cm\\267>\\004\\177\\213\\276!\\236\\277\\276k3\\267\\276L\\364\\341\\273\\216L\\226>\\224<w<0\\034=>\\356?\\332>o\\332\\376=`]\\227;k\\333\\327>@\\337\\237\\276}\\035\\226>\\265\\357\\313>\\250%\\264<$tJ\\274`\\235\\001\\277\\241b\\024\\275C\\320\\215\\275\\242\\001`\\276O\\324p<\\037\\034\\343\\276\\003\\017k\\276N4\\211>a\\250\\316\\274g0\\266\\276\\033\\206\\224\\276\\313y\\247>E\\352\\351<\\243#\\343<2\\024\\265>\\210(\\210>\\245\\367c>\\201}\\224\\275\\343\\316\\346>(\\032\\023\\276Ys\\205>T\\037\\367>\\261\\2531>6\\347\\002\\277\\021\\217\\274\\276h\\320\\000\\277\\222N\\257\\275\\022\\306\\326<\\226\\255Q\\276\\220\\266\\036\\276\\000k\\275\\275\\n2\\312>\\371a\\006\\276\\322Y \\276QN\\206=\\037\\351\\277>\\234!Y>\\322\\373\\356>\\257L\\245>\\275G\\270>\\266\\\\\\315>\\316Bm>\\375b\\220>\\252\\215\\212\\275\\036i\\213=\\376\\026\\177>o\\234\\265>\"\n  }\n  initializer {\n    dims: 150\n    data_type: 1\n    name: \"3.bias\"\n    raw_data: \"\\355\\013]=\\277\\330\\005\\276+\\264\\375<\\177]&>\\010s+\\276j]\\263=\\265eD>\\263J\\261\\275N\\022L=\\337\\263\\352=6\\374\\364\\275\\342\\300\\247\\274.\\303\\231=`\\233\\242\\275U\\266\\256<\\231\\177\\241=\\203\\205\\227\\275sb\\315=\\331\\3402>\\213#\\027\\276\\303Q\\312=t\\214\\021=\\007]\\247\\275\\325\\224\\r<t^\\246=\\024j\\010\\2760\\342\\360\\274\\017M*>|\\333j\\275\\205X\\252=\\374\\271F=\\225\\025\\202\\275\\370\\022\\275=:\\260A>\\2477]\\276\\240Y\\255\\273\\344^&>\\262a\\315\\275H^O=\\032\\255g=\\360G\\007\\276\\222}6\\275m\\366\\352=\\023\\001\\213\\275y\\t\\203=\\321o\\332=\\016\\020/\\276\\031(\\027=?(X>\\320\\254\\r\\276a\\372p=E\\020 >H\\303)\\275Y\\257\\310=\\264\\023{>B\\362<\\276\\234\\234\\255=\\257Y\\032>\\ng<\\2762\\nt\\275V79>\\361\\211\\013\\276gA\\271;\\'ut>KZ\\306\\275e\\2017<I\\237\\203>J\\350\\366\\275\\337{\\203=\\3609\\017>\\235e+\\276cQf\\275%ol>\\341\\330-\\276\\266\\236T\\274\\322\\032\\037>O\\030\\304\\275}\\266\\007=\\035gy>\\377\\023\\265\\275FM\\235=\\332lz>\\315Y\\236\\275\\2630\\245=\\000\\\"U>\\014\\312\\321\\275&\\2543\\274z\\261c>\\024\\305\\t\\276b\\377\\261<\\266)%>\\274w\\337\\274/\\341P<\\324\\262\\225>\\021-p\\275\\372\\223/<\\305\\227\\221>\\215\\3316\\276\\340\\013==b\\263y>\\\\~C\\276\\\\\\225\\356<\\010t\\213>\\253!=\\276\\212m\\230\\275F$\\232>j1\\002\\276r\\214\\021=(8:>\\013\\363\\346\\275\\235\\373\\031\\272\\000\\236\\225>\\0072=\\276\\014\\323\\361=}\\207\\225>\\270\\250\\016\\276\\262\\371\\031=IJ\\231>\\205\\366L\\276\\211\\271\\342\\274\\303\\300\\262>\\371\\006\\010\\276\\373!\\322=VH\\234>*}\\311\\27509G=\\247\\216o>\\345\\352[\\275\\321\\267\\256\\274T)\\251>F\\340\\350\\275q\\003M\\274\\210\\365g>$\\334\\236\\275\\371\\306\\005\\275T\\177r>\\016D\\345\\275\\345\\207)=|^\\253>f3\\324\\275\\272\\311\\024=\\321wK>\\227d\\250\\275\\321\\333B<48\\220>\\026\\204\\267\\275\\371\\224\\243=\\270\\217\\263>=\\210\\376\\275\\326/\\244\\274\"\n  }\n  initializer {\n    dims: 150\n    dims: 6\n    data_type: 1\n    name: \"3.weight\"\n    raw_data: \".\\334\\216>\\320K\\004>R\\234\\327\\275\\n\\366E\\276\\264t\\023?\\352\\201\\334\\276\\3134\\354<\\016\\315=\\276q8\\304\\276\\216xu\\276\\212\\263\\326\\276\\276\\213\\221\\276nao>7\\216\\224=\\023-\\325=\\226V\\250=B\\206\\030\\276:\\225)=S\\231\\314>\\241\\273.>\\334\\3376=lv\\016=\\246{\\323>\\311\\274\\010\\277\\331\\267o<\\306\\272\\205\\276bq\\001\\277\\311\\236\\267\\276\\254\\333\\236\\276\\214\\302 \\276!\\2219\\276\\0247D\\276i\\347\\306\\275\\255\\303\\203>\\t\\261\\213=\\215\\327g\\276\\314\\204\\265=%Ug=j\\371\\010\\275\\372\\273\\233=\\312\\007\\005? X7\\277\\251\\261\\216>T\\013\\344\\276:\\335\\010\\277s\\246\\200=\\202\\035\\337\\276q\\252w\\276\\350\\031\\307>\\266\\371\\215=\\224\\315\\021>(EF>\\\"\\346r\\276\\006JF=\\rmE?aN\\371=\\376\\371\\016\\274\\221\\003\\235\\275\\010S\\233>^\\223h\\276\\3454;\\274\\314@\\\\>\\3528\\216\\273c\\372\\243\\276\\310l$\\277\\240\\325\\340\\276\\223D\\254\\276=d\\201>S\\252\\021>\\341l\\236\\276\\\\\\021Z=\\2472\\250\\275N\\316\\007?5\\304\\267>\\334 8>E\\334\\262\\276\\375^\\373>~\\021\\327\\275\\230[\\226>\\216\\203F\\276\\260\\345\\225\\276y\\351\\277<\\302\\301\\027\\277\\'\\034\\246\\276\\001c\\354=\\322\\334\\\\\\276\\003\\024=\\276\\273Z\\303=\\2127\\230=\\264\\303\\316=\\305\\305\\315>\\332\\335\\204>e5\\201=\\225W\\303\\276\\335\\336\\304>\\255!\\222\\276M\\314?>\\312\\350U\\2768)\\236\\276\\232\\337\\244<\\3576\\r\\277Ti\\302\\276\\222/\\\"\\276\\307\\271N\\276\\334\\323\\260\\275f\\232\\240>\\376H6=\\200\\220\\201\\276\\2325g>D\\177\\320\\275A\\211\\353\\275\\272\\275\\302<\\240\\177\\023?\\310\\035\\353\\276\\235|\\303>\\000\\372!\\276\\2751\\262\\276\\001n^\\276D\\303\\010\\277\\266\\336\\267\\275\\225}\\252\\276]\\261\\006\\276\\270\\320%\\275\\334\\267x>\\314g\\231=s1\\255\\276\\214\\010\\n?\\276\\350Y>\\322\\356\\204<\\2659\\006\\277^\\314\\354>\\3702|\\271)\\317\\341=\\031\\320\\t\\276n\\221\\204\\276\\017\\240I\\275\\353\\245\\014\\277\\233\\027\\320\\276W)\\226\\274)\\202Y\\275\\2260\\210\\275\\305\\200\\005\\275\\266\\001c=o\\225*=\\267\\275\\346>D/\\310>p\\212~>\\'\\000\\345\\276\\236\\222\\223> /e\\276\\034-\\242\\276\\264\\241\\341=\\244V\\027\\276D$\\327\\276\\035\\265\\340\\276\\207\\n\\373\\276\\034\\336\\237\\275\\200\\001\\277\\273\\003\\035\\251\\275\\242\\330O\\276\\277\\177\\350=\\351\\334\\365=0\\207\\237>\\034>\\242=\\304\\332\\365=\\376\\242\\234\\275\\314\\257\\n?\\343\\372\\224\\276\\335\\345\\204\\274\\2178\\027\\276\\317\\351}\\276\\016\\315Q<A\\273\\007\\277v\\304\\006\\277\\377ws<_}\\031\\275\\'\\032g=\\t\\266\\177>\\207K\\262\\275\\247\\3343\\276#\\341\\342>\\035*\\361=\\244i3\\275O\\257\\014\\277\\013\\351\\242>\\265\\201\\026\\276\\373\\325\\034\\276\\\"\\001\\300\\276\\303x\\364\\2762\\266\\224<>\\324\\255\\276k\\202\\000\\277-\\366B\\276\\362!\\241\\276\\262\\363P\\276)\\033\\235>A[\\010>\\236AX\\276\\346\\226\\250>@8\\335\\275\\305w\\303;\\014\\205\\270<\\362\\004\\006?\\\"\\007\\240\\276\\2114\\372>\\232\\245d>\\032W\\256\\275y\\314\\014\\277\\330v,\\277\\357M}=m\\304\\255\\275\\3204\\241>YQz>T)Q\\276\\036\\366\\322\\275\\034\\344.\\2751\\360\\241>\\360\\370F\\2768S\\013\\276\\201P\\315\\275\\001\\307\\306>O\\344\\301\\276\\000yb=\\274qe\\275<\\340^\\276\\230\\010\\035\\276\\256^\\t\\277\\213\\210\\307\\276\\021\\t{=\\361\\212\\034\\276\\336\\000\\272\\275\\360%1>\\260JI<$\\274\\336\\274\\353<\\r>\\221-\\216\\276Gl\\255\\276\\314X\\005\\277\\017\\276I?Q\\231k<\\376\\361f\\275\\206\\310\\356\\275\\r$\\250\\276\\376\\037\\221\\276\\330\\303\\320\\276\\203\\007\\245\\276=aL\\276\\n\\t\\007\\276\\021kl\\276C\\020\\203\\276@\\325\\202>}5\\032>\\317,r>\\256g\\255>e\\237\\236>:\\020\\370\\276\\231-,>\\213\\262\\301\\276h\\212\\333=\\272\\0258\\276P\\\"\\217\\276\\021\\263\\205<\\215/\\t\\277\\376\\213\\334\\276\\307t\\242=A\\222\\223\\276\\327\\322F\\276\\325\\271\\207>t\\346t=\\326\\212\\301\\274.\\306\\002?\\3471\\314\\274\\013\\323(\\274G\\221\\276\\276c\\260.>\\226\\'S\\276Y2:>]\\000\\020\\276U\\322\\303\\276\\004\\304\\267\\276F\\261\\326\\276Z\\334\\310\\275\\325x\\234=\\310\\016\\306=\\370(\\365=\\265Sf=\\262\\352\\343\\275\\022\\226?\\275\\253\\310\\246>\\305|\\214>nF\\354> v\\316\\275\\253*\\'>\\177A\\337\\276\\275,B=_\\030\\014\\276\\231\\317\\260\\276\\224\\330\\213\\276\\202\\310\\327\\276G\\024\\201\\276\\372fF\\275w!\\220=l\\376\\352=\\356\\017\\346=ik\\253\\275{? \\276\\362\\014X>\\333n@>\\316C\\236>\\273\\310\\250\\276\\376\\002\\245>\\371_\\221\\276\\025\\230\\210=\\332\\232\\016\\276\\277NU\\276\\364\\263\\277=0\\260\\024\\277\\216\\330\\010\\277n~[\\275C\\273@\\276:\\222\\211\\275{X\\250>Nh\\3127\\032wT\\276FT\\353>\\177{\\224\\276\\230%\\243:@\\256!>\\363\\304\\024>\\373\\240\\356\\276\\323\\361z>\\341\\231^\\274_\\324\\216\\276\\341\\020\\333\\276$*\\372\\276]\\260~\\275(L\\224\\275\\014\\377\\256\\275\\363\\004\\324;\\023\\370{>Im\\355\\274\\016\\236O\\276J\\361\\246>\\300\\255\\203\\276\\233\\326\\233\\275t\\351w\\276\\350\\267\\337>f\\263\\001\\276\\024\\341_>\\232\\025\\314\\275\\033\\351\\267\\276+\\030\\321\\276\\243\\236\\333\\276R\\300K\\275\\316\\220\\232\\276+\\203\\252=\\233\\344\\204\\275\\274(\\314\\276\\013&K>\\212`\\305=(\\362\\223=\\226\\373\\254\\275\\021\\236\\014>\\272;\\177\\276\\343\\342\\365>\\235Y\\207\\276&,\\350\\274\\271\\262\\233\\274\\326\\341v\\276Gb\\242\\2760\\201\\351\\276~3\\242\\2763%\\010\\275(\\272\\375;\\326\\220\\206\\274 R\\204\\275)\\314\\002=\\2255\\350<\\344i\\215<vE\\000\\276S\\351-> V\\216\\275Wp\\224>\\221d\\n\\277\\203\\244\\321\\274\\346\\240\\026\\276@\\244\\232\\276\\376\\353\\010\\276\\213v\\344\\276\\342\\357\\313\\276Q5~>\\007\\326\\374\\275\\022\\331\\320\\275\\022\\305\\210=\\022\\261I\\274\\214m$>\\312\\242(\\276\\327f\\231\\275\\344\\322\\203>\\365.\\220\\275\\316G\\277>\\272\\257\\030\\277\\222-/>\\375l\\034=C#\\027\\276\\204rd\\276\\306\\366\\025\\277\\023\\350\\226\\276\\036#4=d.\\321\\275c\\260\\333\\274\\317\\314Q>d\\373\\000\\275\\301v\\266\\275\\241\\t\\010?-\\374\\201\\276\\330\\361\\037\\275J\\216\\247\\276\\025\\342\\000>\\017\\001\\326\\2751\\356\\276>\\324\\262!>\\212\\233\\256\\275B\\350\\301\\276\\333\\035\\'\\277\\377\\337\\316\\275D\\263\\240=L\\216\\226=\\246a\\007\\275\\240U\\221\\276\\311XT=Q\\007y>\\346\\325\\231>\\261[i\\276\\013\\314\\036>F84\\275\\006\\350\\223>\\223\\254\\203\\276\\024Q\\002\\276\\025tc\\274v\\327\\225\\276\\320\\376\\357\\276E_\\270\\276\\327\\327\\200\\276\\007~\\205=hr\\301\\274\\272\\027~\\275u\\324\\312\\275\\007\\274)=&\\366\\007>o0\\204>\\255\\354s\\276Q\\264\\004<9\\376\\301\\276E\\373=>\\203;b\\276F\\r\\303<\\312\\\":=\\241\\364\\363\\275L\\317.\\276\\272\\013\\021\\277\\036Q\\332\\276\\275\\271\\256=z\\222=>\\330\\327H>\\2779\\242<\\377\\003\\\"\\276\\263\\\\c\\275\\010N\\235<\\275\\007X\\276\\2667\\\">\\016\\r\\t\\276\\214\\032\\016>\\352\\242\\014\\277[ \\205\\276\\035\\331\\263\\275{\\306\\203\\276|\\3337\\276?\\350\\311\\276\\257I\\007\\277\\357\\337\\024\\276\\316Tt\\275\\213qC<f\\230B>\\357\\253\\030\\273\\320/\\\\\\276S\\303\\257>y\\352\\347\\276ia\\025<\\330\\356\\214<\\375i\\210>\\014\\023i\\276\\375\\252\\245\\275\\343\\333\\272\\2758ii\\276\\315\\353\\272\\275~\\325\\363\\276\\261\\205\\370\\276\\232.\\002>\\343;\\373;\\302\\255\\326=\\333\\263\\203>\\216K\\033\\2764\\004\\n\\276\\355\\'\\254\\275\\222\\025\\274\\274r\\363\\244>\\275\\247\\275\\276\\366\\035N>\\357\\267\\326\\276\\355\\302\\240>B9Y\\276\\277u\\254\\276t}I\\275\\227\\307\\003\\277\\005,X\\276\\307\\304\\214>\\222\\263\\324=:r\\236=\\363d\\223\\275\\330t\\352\\275\\n%5>l\\315\\\\\\276\\275\\252\\252\\275A\\253\\243>{\\243\\253\\276\\323\\355\\315>\\031\\007\\266\\276f4\\201>\\242\\312\\036>\\363\\277V\\275\\016z\\223\\276\\024F%\\277\\303\\347s\\276\\236+N=\\257l\\333<\\361a\\002=\\211=L<\\261a\\031\\275\\270\\265\\370;\\373\\365\\021>,P\\226\\275\\2357\\214>2\\361\\000\\277@U|>B\\n\\272\\275\\301\\274;>J\\251\\223\\276\\311\\343\\237\\276\\320\\337^>c\\223\\t\\277}\\327\\356\\276B(\\233<\\236y\\201\\274\\247\\245\\273\\274\\r-\\220\\274l\\211Y<v\\315\\n=\\245\\027\\253>\\217\\356\\313\\276\\234\\221E>bY\\325=#\\254\\343=\\334)\\244\\276\\213\\365~>\\214\\320\\334\\276\\001M\\366\\276[M!>\\362@\\333\\276\\264\\025\\233\\276\\376\\220j\\273\\033\\325\\317>\\336\\\\\\267>\\254B\\030\\276\\344\\257X\\276\\331k\\252\\275\\026\\226\\030\\275\\036\\263\\000\\276\\347\\214\\312>\\357\\237\\001\\276\\002\\326q;\\326\\373\\025\\277:\\373\\353;\\323?A\\276*\\201\\347\\276\\334(\\327\\276\\022x\\217\\276\\317z\\330\\275\\350\\336\\036\\276,%\\003\\275<\\275\\350\\273\\305\\355\\205=\\2621\\022=\\342r\\010\\276\\207^\\360>\\335}^\\275\\350 \\000?\\223n)\\276\\354\\322/;\\035\\354\\330\\275\\261m\\302\\275\\374P\\277\\274\\243\\343\\247\\276.\\027\\n\\2775\\210\\237\\276~2\\034\\276f\\276\\273\\275\\226\\312.\\276\\321\\335\\033\\276\\023Jj=z\\275\\356=\\207I\\226\\274n\\030\\271=H\\002\\216\\276\\213*\\234>KE\\367\\275p+\\022>\\022\\364\\250\\276\\336F\\016>\\021\\374\\035\\275US\\237\\276d\\225\\347\\276\\361z\\314\\276^\\373\\213\\275\\n\\356f>\\231\\204a=\\255:}\\275\\350\\264\\235\\276<\\261\\016=6\\314\\271>\\357\\370\\231=hH\\024\\277\\227;\\306=]\\332c=(\\324\\246>\\353K\\211\\276\\267\\254\\005>\\240\\003r\\276\\257\\311\\321\\276%\\331<\\276\\263\\367\\272\\276E\\3669\\276mK6=\\324\\342j\\27659<\\2768\\301\\010>\\207\\311\\260=\\034H\\006=IM\\205\\274g.\\031\\276o\\001\\260>v\\246\\n\\277\\031Z\\300>\\006c\\251<$d\\032>\\335!m\\275\\267\\023g\\276\\303p5\\276\\3426\\000\\2771q\\211\\276\\233Y\\037>\\265\\227i\\275\\316Q\\210\\275y\\277\\242\\274\\033\\244\\251;\\255\\033\\020>Q\\233\\203>\\225\\335\\222\\276\\201h\\307>\\307#j\\275\\340w9<3q\\211\\276\\020_\\013>\\330\\010\\226\\275\\230\\351\\257\\276\\177Q\\344\\276[\\240\\275\\276^1T\\275\\221#N\\276\\307}%\\275\\317[\\251=\\260\\322\\245>\\276\\314n\\275\\367w\\274\\276Q\\252A=\\223\\005\\350\\276\\001|p>\\337c\\252\\275\\224T4>\\030\\306\\223\\276\\212\\277\\356>\\370\\311\\246\\276\\256\\353\\364\\276M\\347\\367\\275\\0212\\326\\2763|\\274<\\352m!\\276\\312Y2\\275\\000\\370\\365\\274\\200q\\023=\\013\\251i=5-\\327\\275f\\314\\236>m\\t\\353\\276,(\\234>e\\230\\033<\\017\\367=>\\'b\\227\\275\\351\\210\\007>\\363QZ\\275\\024\\016\\262\\276Y\\374\\002\\277\\016\\n\\263\\276qX\\325\\273\\364\\324\\002>\\304\\364{>\\2679.>\\243pY\\276\\r\\337\\341\\275\\322\\311\\004>\\272t\\035\\276\\0201\\231\\276\\375Z\\361>pk\\371\\272\\332\\004G=\\026\\016\\r\\277Y\\243Q\\275\\201\\2701=\\245\\2579\\276\\351*\\254\\276x\\032\\341\\276e=\\236\\276\\020\\2715\\275\\235\\267\\240\\274\\013\\331\\301=1d\\232>\\300/\\330\\275\\027\\316\\206\\276l\\344\\027\\276\\366\\033\\256\\276\\367\\370\\274>%\\307;\\276E\\306\\231<\\313\\271\\364\\276Vb\\304\\276C\\002\\213<z~@\\276)\\031\\223\\276\\210h\\263\\276f.\\n\\277\\014\\342\\034>\\222L\\371\\2751\\376f\\275Q^9>\\372L\\031\\275\\245\\317\\373;X\\034M>\\303\\233\\370\\276\\250\\3373>~\\202\\257\\276/\\374\\316<\\267\\212\\311\\275\\260\\217/\\276\\252g\\330\\276\\233\\242\\001\\277\\035\\233f=\\243\\250}\\276\\263\\027\\354\\276^\\026\\327\\273\\014\\266\\306\\275dn\\027\\2763\\276\\003\\276c\\335\\375=w\\242\\030>\\365\\263\\325>\\263\\244\\035\\276\\367D0?f\\030\\211\\274>\\265^\\276`\\251W\\276\\375\\210\\312>\\242\\233\\324\\275\\025\\274|\\276<e\\344\\275\\356\\035\\013\\2779\\\\\\004\\276L\\351&\\276\\204\\254Z\\274I\\326\\226\\275\\247*$\\276\\311q\\374=\\366\\327\\025=\\247\\321\\261>\\363\\266\\347\\276\\014\\332\\231>2\\270\\257\\276\\207\\262\\201>j\\316\\214>X\\347\\260>\\001Ug=\\340\\t]\\275a\\034\\023\\275\\357v+\\277\\252d\\231\\276\\350,\\023\\274\\240\\002\\362=\\345\\266\\006=\\227#m\\276\\371?\\271<\\035\\363\\376=\\352\\316J>\\265\\320\\301\\276\\002\\313\\251>\\307\\300\\324\\276h8`\\275\\323x\\206\\275\\342\\225\\264>oX\\215\\276\\301\\274\\317\\276\\213)\\217\\275L\\234\\330\\276;\\201\\304\\275\\033\\000\\003\\275\\027!t<\\211`\\'=\\270\\202\\211=\\3335\\373\\274y\\277\\247\\275Y\\311^\\276\\222\\211\\017\\277\\'j\\232>\\020\\204\\346\\275\\322\\200\\023>\\362\\202\\300\\276,Ey\\275kZ5\\276\\316\\265\\257\\276\\331\\'3\\276\\202&\\254\\2763W\\243\\276\\352\\221\\326=\\277W\\336\\275`\\337\\207\\275rV\\363=\\205`\\313\\273\\312\\021\\263<i\\372A\\274c)\\321\\276\\371\\023\\217>\\204m!\\277`\\237%>t\\211\\234=\\226}v>\\275Q\\227\\276\\247\\206\\310\\276fcy<X\\023\\320\\276\\226k]\\276Q\\332\\201\\276<da\\275\\261\\343\\257\\275;#\\247\\275\\245\\214\\021>\\202\\304\\200\\275\\023Y\\202\\273m_\\274\\276\\261\\250\\363>\\226\\034\\253\\276P\\242\\371=\\256\\251^\\275\\311 4\\276MU\\227\\276]]\\342\\276\\314\\307\\370\\275\\333:r\\276\\373\\035\\273\\276\\346\\246i\\274T\\236\\014=n\\373\\352=\\267\\304V>o\\301\\342\\2755\\201I\\276\\356\\274l>Z\\312/\\277\\250v\\214>\\\\\\277\\320<)\\362&\\276\\266\\n\\213\\276I\\177\\272>\\347\\254\\207\\276 &\\322\\276\\302\\256\\365\\275\\251\\303\\305\\276\\243k\\375\\274xO\\026\\274J\\354\\237=\\330\\022e<W\\2451\\276i\\256\\312<\\320\\227\\306=\"\n  }\n  input {\n    name: \"input.1\"\n    type {\n      tensor_type {\n        elem_type: 1\n        shape {\n          dim {\n            dim_value: 1\n          }\n          dim {\n            dim_value: 9\n          }\n        }\n      }\n    }\n  }\n  output {\n    name: \"12\"\n    type {\n      tensor_type {\n        elem_type: 1\n        shape {\n          dim {\n            dim_value: 1\n          }\n          dim {\n            dim_value: 150\n          }\n        }\n      }\n    }\n  }\n}\n, (tensor([[ 0.2144,  1.4291, -1.2174, -0.6024,  0.2654,  1.4626,  1.5191, -0.3682,\n          1.2702]]),), <function _create_interpreter_name_lookup_fn.<locals>._get_interpreter_name_for_var at 0x7f17e83d9af0>, True, False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann.ipynb Cell 31\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mmodel.onnx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Convert the model to Torch Script\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(model, dummy_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Save the Torch Script module\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Dados/caiopinho/carmen_lcad/pytorch_treino_validacao_teste_ann.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m script_module\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/jit/_trace.py:778\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    773\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtrace doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt support compiling individual module\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms functions.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    774\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease use trace_module\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    775\u001b[0m     )\n\u001b[1;32m    777\u001b[0m name \u001b[39m=\u001b[39m _qualified_name(func)\n\u001b[0;32m--> 778\u001b[0m traced \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_function_from_trace(\n\u001b[1;32m    779\u001b[0m     name, func, example_inputs, var_lookup_fn, strict, _force_outplace\n\u001b[1;32m    780\u001b[0m )\n\u001b[1;32m    782\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[39mif\u001b[39;00m check_trace:\n",
      "\u001b[0;31mTypeError\u001b[0m: _create_function_from_trace(): incompatible function arguments. The following argument types are supported:\n    1. (arg0: str, arg1: function, arg2: tuple, arg3: function, arg4: bool, arg5: bool) -> torch._C.ScriptFunction\n\nInvoked with: '__torch__.onnx.onnx_ml_pb2.ModelProto', ir_version: 6\nopset_import {\n  version: 9\n}\nproducer_name: \"pytorch\"\nproducer_version: \"1.7\"\ngraph {\n  node {\n    input: \"input.1\"\n    input: \"0.weight\"\n    input: \"0.bias\"\n    output: \"9\"\n    name: \"Gemm_0\"\n    op_type: \"Gemm\"\n    attribute {\n      name: \"alpha\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"beta\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"transB\"\n      type: INT\n      i: 1\n    }\n  }\n  node {\n    input: \"9\"\n    input: \"1.weight\"\n    input: \"1.bias\"\n    output: \"10\"\n    name: \"Gemm_1\"\n    op_type: \"Gemm\"\n    attribute {\n      name: \"alpha\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"beta\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"transB\"\n      type: INT\n      i: 1\n    }\n  }\n  node {\n    input: \"10\"\n    input: \"2.weight\"\n    input: \"2.bias\"\n    output: \"11\"\n    name: \"Gemm_2\"\n    op_type: \"Gemm\"\n    attribute {\n      name: \"alpha\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"beta\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"transB\"\n      type: INT\n      i: 1\n    }\n  }\n  node {\n    input: \"11\"\n    input: \"3.weight\"\n    input: \"3.bias\"\n    output: \"12\"\n    name: \"Gemm_3\"\n    op_type: \"Gemm\"\n    attribute {\n      name: \"alpha\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"beta\"\n      type: FLOAT\n      f: 1\n    }\n    attribute {\n      name: \"transB\"\n      type: INT\n      i: 1\n    }\n  }\n  name: \"torch-jit-export\"\n  initializer {\n    dims: 24\n    data_type: 1\n    name: \"0.bias\"\n    raw_data: \"\\\"\\037\\361>3t\\362>\\203u\\311>,\\371\\342>[\\365}\\276\\323\\310\\227>\\344i\\350\\276\\243\\331\\272\\276F\\032x\\276\\236V\\373>\\306P\\362\\276&\\363 >[\\373\\267>\\216\\213\\324\\276\\204\\010\\222\\276\\024w\\353>y\\026\\223\\276y\\322\\n\\277*+\\t\\277Fnh>\\267\\250\\330\\276\\272\\361\\034?G\\002\\217>0w\\275\\276\"\n  }\n  initializer {\n    dims: 24\n    dims: 9\n    data_type: 1\n    name: \"0.weight\"\n    raw_data: \"\\356R\\027\\276\\213c1\\276\\017dU\\276\\3406U\\276\\366\\347\\307=X\\267C<\\370P\\013\\276$\\0138\\276\\351\\265\\\"\\276\\217\\236\\030>\\0361\\213=\\317\\3456\\276\\034\\022\\331=vM5\\275\\010\\346\\353=\\252\\237\\311<\\310\\0048\\273i1\\203>S>`\\276\\374`\\226\\276\\202\\261\\245=\\005\\027\\214\\276\\327\\300\\253\\275\\203~\\313\\275\\'0Q=\\337\\323w>\\232\\275@>/\\023\\202>\\357\\311u\\274M>]>$*C\\275\\2450\\223\\276z\\344J\\275+\\351\\205=\\204\\313\\004\\276\\366\\223\\371\\274\\322\\215h\\276\\212|\\223>\\365G\\301=\\271\\014g=\\205P\\036>t\\356\\272=\\221\\032\\344\\275\\205\\274\\210\\276V`\\353=\\312Z\\267\\275Y\\005\\217>4U\\360=\\236\\037\\321\\275[\\337\\231<\\307J\\245\\276\\375\\271\\232\\274o\\204$=\\323\\245\\204>\\\\\\305r\\276K\\213\\367=\\013\\247\\202\\276-\\377\\255\\276\\262\\366\\211=(^i>Eu]\\276@\\347O>\\036%\\260\\275\\033\\356Q>\\217\\201\\334\\275\\214\\237\\035>x\\030?>c\\025\\230>\\252]\\265;\\330,\\345\\275\\202\\263G\\276\\024\\202\\206><\\315Y\\2760\\372\\212>^\\264\\230\\276\\336\\343\\345:\\356\\214X\\276\\372\\230}>w\\006J\\276O\\035\\013>\\341\\220\\274;\\2622r\\276\\375M\\232>\\203S\\341=\\244\\260\\037\\276B\\360d>?N\\361=\\000FO>\\014)\\324>i!\\275\\275XM\\'>\\301\\374\\277\\274\\216\\022i>\\0241\\336\\274\\000\\353\\020>\\022%\\025>\\301\\352\\217\\274\\253\\200\\205\\276\\242\\243\\266><.\\274=\\310\\365<=Y\\2443\\276\\310\\017\\360=\\312\\357\\204\\276\\036S\\211>\\237;\\207=\\363Z\\313=\\303\\217\\355=\\371v\\220=e\\332\\203>\\321\\002l>Q]\\332\\273\\325\\227\\002\\276\\300\\3725=|R\\'>x\\277\\326\\275m\\036\\272\\275\\225\\325\\232\\275\\024\\234\\212\\276\\254\\t\\367;\\302\\351\\214\\276\\367\\200E\\276\\002t\\332=\\207\\363z\\276C7\\321\\276Ht\\000;b\\rg>\\225Q$=\\261\\315\\n\\276>\\004J\\276\\241\\315K>\\356F \\276\\207\\n\\323>\\005k\\023\\275\\356\\024t\\276RB\\277=\\037pp<\\004\\210A\\275\\300m\\200\\275\\021\\2229>>[d=\\374z\\323=\\021\\232\\226\\275\\023\\263\\220\\2759\\371\\221\\276\\353m\\202\\276HL?\\276F\\230s>~1\\035\\2769+\\000>\\336\\022\\223>\\010\\017\\330\\276\\004s-\\276\\235\\2522>.\\222\\355=\\220[\\257=\\362e\\313\\276\\274\\366\\314<)\\345\\354=\\255\\000\\231>]\\322(\\276\\205K\\233\\276\\205\\027\\253=\\224\\351\\312\\275:\\321D>\\254~Q\\275+\\321u\\276\\016\\377\\021\\276@\\352o\\275\\275\\330?>7)c>7H\\230\\275\\353\\346M\\276\\025\\222)=\\371\\315w\\276p\\327V\\276V \\r=j\\030B\\275g\\216\\232>:Z\\307=y\\314\\200\\275\\025\\274\\360\\275\\362-;\\275s\\303\\221\\276\\261\\224\\344\\275\\321Uu\\275\\274\\\"\\376<;\\021\\204\\276\\346i\\253\\273\\r\\275[=[\\026\\314\\275gm\\237\\273\\257\\317X>\\263\\374\\203>s\\005+>?\\240\\225\\276\\013\\312\\215>\\025-\\275=!\\331w\\275\\031C\\014=\\r\\372\\344\\275}\\374?\\276\\346\\240\\216\\276\\226\\361\\000=I\\310\\316\\276\\271-\\020>I\\251\\366\\275\\005\\361\\203\\276.\\243\\251>\\375\\027\\003\\274\\273\\237\\001>&\\246\\001\\274\\331\\327|\\274\\334\\362\\033\\276\\360\\234\\323\\276\\034\\317\\305\\275\"\n  }\n  initializer {\n    dims: 12\n    data_type: 1\n    name: \"1.bias\"\n    raw_data: \"\\t\\021\\325\\2765\\343 \\276\\201=\\323\\276E\\266\\237\\276\\230\\317\\263\\276\\002\\t\\322\\276\\245\\372$\\276pd\\230\\276\\307\\267\\257= \\342\\241\\276\\227\\363\\302\\276\\n\\321\\303\\276\"\n  }\n  initializer {\n    dims: 12\n    dims: 24\n    data_type: 1\n    name: \"1.weight\"\n    raw_data: \"\\250?\\037\\276\\020S\\225\\276!>?\\276\\313\\320\\016\\276\\345\\315\\234>\\331l\\013\\276\\277L&>tb\\201>2[\\031>RT\\312\\276\\323\\r\\034>\\220\\035\\002\\276\\315=\\217\\276\\254\\232\\227>\\241\\363$>%\\203\\265\\276\\262\\014\\221>\\214\\272\\367<\\3677\\366=KC\\020\\276t\\365\\235\\275\\246\\326+\\276\\022\\341\\251\\276\\037\\031\\323>\\373\\020\\177\\276\\312\\227Q\\276Ku\\363\\275\\371l\\255\\275\\r\\364y>R\\256>\\2766\\373\\317>\\014\\033\\311>\\203\\311\\241<\\027q\\031\\276z2\\247>\\367\\312\\346\\275\\377\\262\\324\\276\\334\\037\\302=\\035\\345\\025>\\322r|\\276\\353\\003\\220\\275\\243\\273\\264>\\254\\347\\002>\\335\\254\\214=|\\356\\253>mm\\265\\276\\245\\252j\\276#M_>yU\\252\\275\\334\\205d\\276\\036s\\225\\276\\255\\212\\314\\275\\254\\222\\014>\\256\\363\\323\\275\\346\\302$> \\372@>\\222BP>\\267\\205\\243\\276\\024\\002\\217=V F\\275\\230\\016\\031<.+\\370=^\\310\\276;\\034I\\320\\274\\201\\340\\250>\\222\\024\\254<\\\\\\t6>m\\250\\203\\276\\340B_>/\\277\\215\\276\\325\\311\\210\\276m\\226s=77\\031\\275\\023\\3351\\275\\026ia\\276\\304\\247\\221\\276\\222=\\353=\\241h\\216=9aB=\\335<;<{w\\321;\\032NQ\\276\\202\\352\\006=bU\\215\\2751\\211\\247\\276:* >n\\023 =\\233qt\\276\\211;\\231\\274\\212\\024\\220>\\343\\247R>_A|\\276\\t\\315\\024>D\\207\\253\\274\\321b\\224\\276\\022t\\266>\\372\\2539\\276\\263\\260{\\276\\315\\242\\207\\275f\\324\\034\\276A\\252\\225=12\\023\\275g\\351\\310>\\341(\\334=\\242tg\\275{\\016K\\276O^B>\\010H\\260\\275mc\\234\\276y\\373O>\\214\\3437>c\\374q\\276\\254\\370h>h\\352\\212=^\\310\\036=\\214\\315\\276\\275\\020R\\264>D\\236\\303\\275\\351\\304\\332\\275\\025\\277\\254>D\\241\\236\\276^r\\251\\276\\375o\\231<|j\\352\\275y\\271\\257\\275\\222\\031\\024\\276$\\t\\016=v\\271\\246>\\030\\205J\\273N\\017\\\\\\276:\\217\\217>#v\\032\\276\\257dB\\2765{\\206<T\\337\\364\\275\\022R=\\275\\275M*=\\263\\025\\214>!&\\207>\\317\\211\\200\\276\\304\\314\\236:\\206<\\\"\\276f\\362\\227\\276\\210cQ>B\\372\\'\\276L\\013d\\276(K\\364\\275\\361C\\372\\275\\234\\364\\367=\\273w\\243\\275\\303\\350S>\\207\\226\\234>\\276Z\\314=\\356l\\261\\275\\226%\\325>~\\020.>\\373\\260\\303\\276\\257\\273\\206>\\344\\223\\r=M\\247[\\276\\032\\237\\233>\\276@\\217<P\\212\\216>\\026\\267t\\275\\200:\\261>\\367\\311}\\276\\243\\322/\\275\\377 ?>\\274\\377+\\276|\\'\\205\\276\\372\\337c\\2756\\202\\370\\276\\302\\377~\\275;\\261]\\276\\024\\302\\326=!\\001\\036>\\274\\221S>*\\346\\256\\274\\\\=\\203>\\227\\021\\014=>\\217\\027\\275\\227\\361\\307>#\\305\\032>\\340v\\230\\276\\360\\250\\232\\275\\303b\\313>nv\\251>\\010tZ\\274\\223\\\\\\246=\\276\\314\\301\\2762\\247\\204=*\\376];\\343\\364m>\\327Bj>\\256Sn>\\370\\346\\256>\\3337\\340\\2753M\\364\\272\\232\\022\\346\\275J\\276\\276\\275\\252\\237\\006\\276\\255\\220\\353=\\013\\270\\017\\276\\035\\022S>\\274\\210Q>\\342>)\\276\\345OS\\276\\376\\007M=\\240\\212e<O\\323)\\276\\202(\\277\\276\\371\\261v>\\374!\\244\\276\\355}\\263>\\\\\\365\\022>\\240\\330D\\276\\216\\177+\\275\\257=\\005\\276\\260\\222\\333\\275\\353\\356\\232\\276I\\212L>\\271c\\255=&19><\\240;=\\330oh>\\3216_\\276A\\202\\262>\\037\\351s\\274\\2624\\231=X\\241\\367=\\367\\232\\233\\272\\256\\357\\004\\276\\037n/>\\020\\375\\252> \\273\\355=\\233\\275\\000\\275Q\\360m>o\\030/\\274K\\200\\271<\\376lp>\\2528\\325<:\\273\\271\\274k\\346|\\275\\231A\\261\\275\\324\\'6\\275\\014\\330?\\2743\\235\\250<\\246\\213\\265>\\035-Q>?\\364\\252\\276-\\257\\215>]CI\\275\\220\\005\\t\\276qv\\356<\\\\\\337d>\\262\\316D\\276\\177\\277\\205>\\035o\\203>\\342<\\271>{\\303\\212=!t;>\\300\\313E\\2766bH\\276\\035\\360\\017>\\346}*\\276\\361o\\246\\276l\\355k\\276v\\323\\252\\276\\024\\230\\304=\\263\\\"P\\276\\004\\370]>v\\357\\263<\\253v\\\"\\276\\315N\\332\\275\\372\\037\\211>\\017\\222\\340;A\\031>\\275\\265tt\\275\\237uI=\\355\\203,\\276\\261\\321\\232=\\202h\\237>\\266\\232\\322=\\261\\325u\\276\\332Wa>;\\003]\\276\\004\\216M=\\014\\253\\243>\"\n  }\n  initializer {\n    dims: 6\n    data_type: 1\n    name: \"2.bias\"\n    raw_data: \"\\240\\240\\353>\\251\\303\\313\\276N\\\"\\304>\\361\\216\\306\\2764\\331\\237>\\271\\331X\\276\"\n  }\n  initializer {\n    dims: 6\n    dims: 12\n    data_type: 1\n    name: \"2.weight\"\n    raw_data: \"+\\254\\230\\2763\\342\\245\\273n\\245\\345\\275E\\230Q\\276\\257\\262\\274\\276\\243\\000\\243\\276\\330qG\\276\\274%v=Cm\\267>\\004\\177\\213\\276!\\236\\277\\276k3\\267\\276L\\364\\341\\273\\216L\\226>\\224<w<0\\034=>\\356?\\332>o\\332\\376=`]\\227;k\\333\\327>@\\337\\237\\276}\\035\\226>\\265\\357\\313>\\250%\\264<$tJ\\274`\\235\\001\\277\\241b\\024\\275C\\320\\215\\275\\242\\001`\\276O\\324p<\\037\\034\\343\\276\\003\\017k\\276N4\\211>a\\250\\316\\274g0\\266\\276\\033\\206\\224\\276\\313y\\247>E\\352\\351<\\243#\\343<2\\024\\265>\\210(\\210>\\245\\367c>\\201}\\224\\275\\343\\316\\346>(\\032\\023\\276Ys\\205>T\\037\\367>\\261\\2531>6\\347\\002\\277\\021\\217\\274\\276h\\320\\000\\277\\222N\\257\\275\\022\\306\\326<\\226\\255Q\\276\\220\\266\\036\\276\\000k\\275\\275\\n2\\312>\\371a\\006\\276\\322Y \\276QN\\206=\\037\\351\\277>\\234!Y>\\322\\373\\356>\\257L\\245>\\275G\\270>\\266\\\\\\315>\\316Bm>\\375b\\220>\\252\\215\\212\\275\\036i\\213=\\376\\026\\177>o\\234\\265>\"\n  }\n  initializer {\n    dims: 150\n    data_type: 1\n    name: \"3.bias\"\n    raw_data: \"\\355\\013]=\\277\\330\\005\\276+\\264\\375<\\177]&>\\010s+\\276j]\\263=\\265eD>\\263J\\261\\275N\\022L=\\337\\263\\352=6\\374\\364\\275\\342\\300\\247\\274.\\303\\231=`\\233\\242\\275U\\266\\256<\\231\\177\\241=\\203\\205\\227\\275sb\\315=\\331\\3402>\\213#\\027\\276\\303Q\\312=t\\214\\021=\\007]\\247\\275\\325\\224\\r<t^\\246=\\024j\\010\\2760\\342\\360\\274\\017M*>|\\333j\\275\\205X\\252=\\374\\271F=\\225\\025\\202\\275\\370\\022\\275=:\\260A>\\2477]\\276\\240Y\\255\\273\\344^&>\\262a\\315\\275H^O=\\032\\255g=\\360G\\007\\276\\222}6\\275m\\366\\352=\\023\\001\\213\\275y\\t\\203=\\321o\\332=\\016\\020/\\276\\031(\\027=?(X>\\320\\254\\r\\276a\\372p=E\\020 >H\\303)\\275Y\\257\\310=\\264\\023{>B\\362<\\276\\234\\234\\255=\\257Y\\032>\\ng<\\2762\\nt\\275V79>\\361\\211\\013\\276gA\\271;\\'ut>KZ\\306\\275e\\2017<I\\237\\203>J\\350\\366\\275\\337{\\203=\\3609\\017>\\235e+\\276cQf\\275%ol>\\341\\330-\\276\\266\\236T\\274\\322\\032\\037>O\\030\\304\\275}\\266\\007=\\035gy>\\377\\023\\265\\275FM\\235=\\332lz>\\315Y\\236\\275\\2630\\245=\\000\\\"U>\\014\\312\\321\\275&\\2543\\274z\\261c>\\024\\305\\t\\276b\\377\\261<\\266)%>\\274w\\337\\274/\\341P<\\324\\262\\225>\\021-p\\275\\372\\223/<\\305\\227\\221>\\215\\3316\\276\\340\\013==b\\263y>\\\\~C\\276\\\\\\225\\356<\\010t\\213>\\253!=\\276\\212m\\230\\275F$\\232>j1\\002\\276r\\214\\021=(8:>\\013\\363\\346\\275\\235\\373\\031\\272\\000\\236\\225>\\0072=\\276\\014\\323\\361=}\\207\\225>\\270\\250\\016\\276\\262\\371\\031=IJ\\231>\\205\\366L\\276\\211\\271\\342\\274\\303\\300\\262>\\371\\006\\010\\276\\373!\\322=VH\\234>*}\\311\\27509G=\\247\\216o>\\345\\352[\\275\\321\\267\\256\\274T)\\251>F\\340\\350\\275q\\003M\\274\\210\\365g>$\\334\\236\\275\\371\\306\\005\\275T\\177r>\\016D\\345\\275\\345\\207)=|^\\253>f3\\324\\275\\272\\311\\024=\\321wK>\\227d\\250\\275\\321\\333B<48\\220>\\026\\204\\267\\275\\371\\224\\243=\\270\\217\\263>=\\210\\376\\275\\326/\\244\\274\"\n  }\n  initializer {\n    dims: 150\n    dims: 6\n    data_type: 1\n    name: \"3.weight\"\n    raw_data: \".\\334\\216>\\320K\\004>R\\234\\327\\275\\n\\366E\\276\\264t\\023?\\352\\201\\334\\276\\3134\\354<\\016\\315=\\276q8\\304\\276\\216xu\\276\\212\\263\\326\\276\\276\\213\\221\\276nao>7\\216\\224=\\023-\\325=\\226V\\250=B\\206\\030\\276:\\225)=S\\231\\314>\\241\\273.>\\334\\3376=lv\\016=\\246{\\323>\\311\\274\\010\\277\\331\\267o<\\306\\272\\205\\276bq\\001\\277\\311\\236\\267\\276\\254\\333\\236\\276\\214\\302 \\276!\\2219\\276\\0247D\\276i\\347\\306\\275\\255\\303\\203>\\t\\261\\213=\\215\\327g\\276\\314\\204\\265=%Ug=j\\371\\010\\275\\372\\273\\233=\\312\\007\\005? X7\\277\\251\\261\\216>T\\013\\344\\276:\\335\\010\\277s\\246\\200=\\202\\035\\337\\276q\\252w\\276\\350\\031\\307>\\266\\371\\215=\\224\\315\\021>(EF>\\\"\\346r\\276\\006JF=\\rmE?aN\\371=\\376\\371\\016\\274\\221\\003\\235\\275\\010S\\233>^\\223h\\276\\3454;\\274\\314@\\\\>\\3528\\216\\273c\\372\\243\\276\\310l$\\277\\240\\325\\340\\276\\223D\\254\\276=d\\201>S\\252\\021>\\341l\\236\\276\\\\\\021Z=\\2472\\250\\275N\\316\\007?5\\304\\267>\\334 8>E\\334\\262\\276\\375^\\373>~\\021\\327\\275\\230[\\226>\\216\\203F\\276\\260\\345\\225\\276y\\351\\277<\\302\\301\\027\\277\\'\\034\\246\\276\\001c\\354=\\322\\334\\\\\\276\\003\\024=\\276\\273Z\\303=\\2127\\230=\\264\\303\\316=\\305\\305\\315>\\332\\335\\204>e5\\201=\\225W\\303\\276\\335\\336\\304>\\255!\\222\\276M\\314?>\\312\\350U\\2768)\\236\\276\\232\\337\\244<\\3576\\r\\277Ti\\302\\276\\222/\\\"\\276\\307\\271N\\276\\334\\323\\260\\275f\\232\\240>\\376H6=\\200\\220\\201\\276\\2325g>D\\177\\320\\275A\\211\\353\\275\\272\\275\\302<\\240\\177\\023?\\310\\035\\353\\276\\235|\\303>\\000\\372!\\276\\2751\\262\\276\\001n^\\276D\\303\\010\\277\\266\\336\\267\\275\\225}\\252\\276]\\261\\006\\276\\270\\320%\\275\\334\\267x>\\314g\\231=s1\\255\\276\\214\\010\\n?\\276\\350Y>\\322\\356\\204<\\2659\\006\\277^\\314\\354>\\3702|\\271)\\317\\341=\\031\\320\\t\\276n\\221\\204\\276\\017\\240I\\275\\353\\245\\014\\277\\233\\027\\320\\276W)\\226\\274)\\202Y\\275\\2260\\210\\275\\305\\200\\005\\275\\266\\001c=o\\225*=\\267\\275\\346>D/\\310>p\\212~>\\'\\000\\345\\276\\236\\222\\223> /e\\276\\034-\\242\\276\\264\\241\\341=\\244V\\027\\276D$\\327\\276\\035\\265\\340\\276\\207\\n\\373\\276\\034\\336\\237\\275\\200\\001\\277\\273\\003\\035\\251\\275\\242\\330O\\276\\277\\177\\350=\\351\\334\\365=0\\207\\237>\\034>\\242=\\304\\332\\365=\\376\\242\\234\\275\\314\\257\\n?\\343\\372\\224\\276\\335\\345\\204\\274\\2178\\027\\276\\317\\351}\\276\\016\\315Q<A\\273\\007\\277v\\304\\006\\277\\377ws<_}\\031\\275\\'\\032g=\\t\\266\\177>\\207K\\262\\275\\247\\3343\\276#\\341\\342>\\035*\\361=\\244i3\\275O\\257\\014\\277\\013\\351\\242>\\265\\201\\026\\276\\373\\325\\034\\276\\\"\\001\\300\\276\\303x\\364\\2762\\266\\224<>\\324\\255\\276k\\202\\000\\277-\\366B\\276\\362!\\241\\276\\262\\363P\\276)\\033\\235>A[\\010>\\236AX\\276\\346\\226\\250>@8\\335\\275\\305w\\303;\\014\\205\\270<\\362\\004\\006?\\\"\\007\\240\\276\\2114\\372>\\232\\245d>\\032W\\256\\275y\\314\\014\\277\\330v,\\277\\357M}=m\\304\\255\\275\\3204\\241>YQz>T)Q\\276\\036\\366\\322\\275\\034\\344.\\2751\\360\\241>\\360\\370F\\2768S\\013\\276\\201P\\315\\275\\001\\307\\306>O\\344\\301\\276\\000yb=\\274qe\\275<\\340^\\276\\230\\010\\035\\276\\256^\\t\\277\\213\\210\\307\\276\\021\\t{=\\361\\212\\034\\276\\336\\000\\272\\275\\360%1>\\260JI<$\\274\\336\\274\\353<\\r>\\221-\\216\\276Gl\\255\\276\\314X\\005\\277\\017\\276I?Q\\231k<\\376\\361f\\275\\206\\310\\356\\275\\r$\\250\\276\\376\\037\\221\\276\\330\\303\\320\\276\\203\\007\\245\\276=aL\\276\\n\\t\\007\\276\\021kl\\276C\\020\\203\\276@\\325\\202>}5\\032>\\317,r>\\256g\\255>e\\237\\236>:\\020\\370\\276\\231-,>\\213\\262\\301\\276h\\212\\333=\\272\\0258\\276P\\\"\\217\\276\\021\\263\\205<\\215/\\t\\277\\376\\213\\334\\276\\307t\\242=A\\222\\223\\276\\327\\322F\\276\\325\\271\\207>t\\346t=\\326\\212\\301\\274.\\306\\002?\\3471\\314\\274\\013\\323(\\274G\\221\\276\\276c\\260.>\\226\\'S\\276Y2:>]\\000\\020\\276U\\322\\303\\276\\004\\304\\267\\276F\\261\\326\\276Z\\334\\310\\275\\325x\\234=\\310\\016\\306=\\370(\\365=\\265Sf=\\262\\352\\343\\275\\022\\226?\\275\\253\\310\\246>\\305|\\214>nF\\354> v\\316\\275\\253*\\'>\\177A\\337\\276\\275,B=_\\030\\014\\276\\231\\317\\260\\276\\224\\330\\213\\276\\202\\310\\327\\276G\\024\\201\\276\\372fF\\275w!\\220=l\\376\\352=\\356\\017\\346=ik\\253\\275{? \\276\\362\\014X>\\333n@>\\316C\\236>\\273\\310\\250\\276\\376\\002\\245>\\371_\\221\\276\\025\\230\\210=\\332\\232\\016\\276\\277NU\\276\\364\\263\\277=0\\260\\024\\277\\216\\330\\010\\277n~[\\275C\\273@\\276:\\222\\211\\275{X\\250>Nh\\3127\\032wT\\276FT\\353>\\177{\\224\\276\\230%\\243:@\\256!>\\363\\304\\024>\\373\\240\\356\\276\\323\\361z>\\341\\231^\\274_\\324\\216\\276\\341\\020\\333\\276$*\\372\\276]\\260~\\275(L\\224\\275\\014\\377\\256\\275\\363\\004\\324;\\023\\370{>Im\\355\\274\\016\\236O\\276J\\361\\246>\\300\\255\\203\\276\\233\\326\\233\\275t\\351w\\276\\350\\267\\337>f\\263\\001\\276\\024\\341_>\\232\\025\\314\\275\\033\\351\\267\\276+\\030\\321\\276\\243\\236\\333\\276R\\300K\\275\\316\\220\\232\\276+\\203\\252=\\233\\344\\204\\275\\274(\\314\\276\\013&K>\\212`\\305=(\\362\\223=\\226\\373\\254\\275\\021\\236\\014>\\272;\\177\\276\\343\\342\\365>\\235Y\\207\\276&,\\350\\274\\271\\262\\233\\274\\326\\341v\\276Gb\\242\\2760\\201\\351\\276~3\\242\\2763%\\010\\275(\\272\\375;\\326\\220\\206\\274 R\\204\\275)\\314\\002=\\2255\\350<\\344i\\215<vE\\000\\276S\\351-> V\\216\\275Wp\\224>\\221d\\n\\277\\203\\244\\321\\274\\346\\240\\026\\276@\\244\\232\\276\\376\\353\\010\\276\\213v\\344\\276\\342\\357\\313\\276Q5~>\\007\\326\\374\\275\\022\\331\\320\\275\\022\\305\\210=\\022\\261I\\274\\214m$>\\312\\242(\\276\\327f\\231\\275\\344\\322\\203>\\365.\\220\\275\\316G\\277>\\272\\257\\030\\277\\222-/>\\375l\\034=C#\\027\\276\\204rd\\276\\306\\366\\025\\277\\023\\350\\226\\276\\036#4=d.\\321\\275c\\260\\333\\274\\317\\314Q>d\\373\\000\\275\\301v\\266\\275\\241\\t\\010?-\\374\\201\\276\\330\\361\\037\\275J\\216\\247\\276\\025\\342\\000>\\017\\001\\326\\2751\\356\\276>\\324\\262!>\\212\\233\\256\\275B\\350\\301\\276\\333\\035\\'\\277\\377\\337\\316\\275D\\263\\240=L\\216\\226=\\246a\\007\\275\\240U\\221\\276\\311XT=Q\\007y>\\346\\325\\231>\\261[i\\276\\013\\314\\036>F84\\275\\006\\350\\223>\\223\\254\\203\\276\\024Q\\002\\276\\025tc\\274v\\327\\225\\276\\320\\376\\357\\276E_\\270\\276\\327\\327\\200\\276\\007~\\205=hr\\301\\274\\272\\027~\\275u\\324\\312\\275\\007\\274)=&\\366\\007>o0\\204>\\255\\354s\\276Q\\264\\004<9\\376\\301\\276E\\373=>\\203;b\\276F\\r\\303<\\312\\\":=\\241\\364\\363\\275L\\317.\\276\\272\\013\\021\\277\\036Q\\332\\276\\275\\271\\256=z\\222=>\\330\\327H>\\2779\\242<\\377\\003\\\"\\276\\263\\\\c\\275\\010N\\235<\\275\\007X\\276\\2667\\\">\\016\\r\\t\\276\\214\\032\\016>\\352\\242\\014\\277[ \\205\\276\\035\\331\\263\\275{\\306\\203\\276|\\3337\\276?\\350\\311\\276\\257I\\007\\277\\357\\337\\024\\276\\316Tt\\275\\213qC<f\\230B>\\357\\253\\030\\273\\320/\\\\\\276S\\303\\257>y\\352\\347\\276ia\\025<\\330\\356\\214<\\375i\\210>\\014\\023i\\276\\375\\252\\245\\275\\343\\333\\272\\2758ii\\276\\315\\353\\272\\275~\\325\\363\\276\\261\\205\\370\\276\\232.\\002>\\343;\\373;\\302\\255\\326=\\333\\263\\203>\\216K\\033\\2764\\004\\n\\276\\355\\'\\254\\275\\222\\025\\274\\274r\\363\\244>\\275\\247\\275\\276\\366\\035N>\\357\\267\\326\\276\\355\\302\\240>B9Y\\276\\277u\\254\\276t}I\\275\\227\\307\\003\\277\\005,X\\276\\307\\304\\214>\\222\\263\\324=:r\\236=\\363d\\223\\275\\330t\\352\\275\\n%5>l\\315\\\\\\276\\275\\252\\252\\275A\\253\\243>{\\243\\253\\276\\323\\355\\315>\\031\\007\\266\\276f4\\201>\\242\\312\\036>\\363\\277V\\275\\016z\\223\\276\\024F%\\277\\303\\347s\\276\\236+N=\\257l\\333<\\361a\\002=\\211=L<\\261a\\031\\275\\270\\265\\370;\\373\\365\\021>,P\\226\\275\\2357\\214>2\\361\\000\\277@U|>B\\n\\272\\275\\301\\274;>J\\251\\223\\276\\311\\343\\237\\276\\320\\337^>c\\223\\t\\277}\\327\\356\\276B(\\233<\\236y\\201\\274\\247\\245\\273\\274\\r-\\220\\274l\\211Y<v\\315\\n=\\245\\027\\253>\\217\\356\\313\\276\\234\\221E>bY\\325=#\\254\\343=\\334)\\244\\276\\213\\365~>\\214\\320\\334\\276\\001M\\366\\276[M!>\\362@\\333\\276\\264\\025\\233\\276\\376\\220j\\273\\033\\325\\317>\\336\\\\\\267>\\254B\\030\\276\\344\\257X\\276\\331k\\252\\275\\026\\226\\030\\275\\036\\263\\000\\276\\347\\214\\312>\\357\\237\\001\\276\\002\\326q;\\326\\373\\025\\277:\\373\\353;\\323?A\\276*\\201\\347\\276\\334(\\327\\276\\022x\\217\\276\\317z\\330\\275\\350\\336\\036\\276,%\\003\\275<\\275\\350\\273\\305\\355\\205=\\2621\\022=\\342r\\010\\276\\207^\\360>\\335}^\\275\\350 \\000?\\223n)\\276\\354\\322/;\\035\\354\\330\\275\\261m\\302\\275\\374P\\277\\274\\243\\343\\247\\276.\\027\\n\\2775\\210\\237\\276~2\\034\\276f\\276\\273\\275\\226\\312.\\276\\321\\335\\033\\276\\023Jj=z\\275\\356=\\207I\\226\\274n\\030\\271=H\\002\\216\\276\\213*\\234>KE\\367\\275p+\\022>\\022\\364\\250\\276\\336F\\016>\\021\\374\\035\\275US\\237\\276d\\225\\347\\276\\361z\\314\\276^\\373\\213\\275\\n\\356f>\\231\\204a=\\255:}\\275\\350\\264\\235\\276<\\261\\016=6\\314\\271>\\357\\370\\231=hH\\024\\277\\227;\\306=]\\332c=(\\324\\246>\\353K\\211\\276\\267\\254\\005>\\240\\003r\\276\\257\\311\\321\\276%\\331<\\276\\263\\367\\272\\276E\\3669\\276mK6=\\324\\342j\\27659<\\2768\\301\\010>\\207\\311\\260=\\034H\\006=IM\\205\\274g.\\031\\276o\\001\\260>v\\246\\n\\277\\031Z\\300>\\006c\\251<$d\\032>\\335!m\\275\\267\\023g\\276\\303p5\\276\\3426\\000\\2771q\\211\\276\\233Y\\037>\\265\\227i\\275\\316Q\\210\\275y\\277\\242\\274\\033\\244\\251;\\255\\033\\020>Q\\233\\203>\\225\\335\\222\\276\\201h\\307>\\307#j\\275\\340w9<3q\\211\\276\\020_\\013>\\330\\010\\226\\275\\230\\351\\257\\276\\177Q\\344\\276[\\240\\275\\276^1T\\275\\221#N\\276\\307}%\\275\\317[\\251=\\260\\322\\245>\\276\\314n\\275\\367w\\274\\276Q\\252A=\\223\\005\\350\\276\\001|p>\\337c\\252\\275\\224T4>\\030\\306\\223\\276\\212\\277\\356>\\370\\311\\246\\276\\256\\353\\364\\276M\\347\\367\\275\\0212\\326\\2763|\\274<\\352m!\\276\\312Y2\\275\\000\\370\\365\\274\\200q\\023=\\013\\251i=5-\\327\\275f\\314\\236>m\\t\\353\\276,(\\234>e\\230\\033<\\017\\367=>\\'b\\227\\275\\351\\210\\007>\\363QZ\\275\\024\\016\\262\\276Y\\374\\002\\277\\016\\n\\263\\276qX\\325\\273\\364\\324\\002>\\304\\364{>\\2679.>\\243pY\\276\\r\\337\\341\\275\\322\\311\\004>\\272t\\035\\276\\0201\\231\\276\\375Z\\361>pk\\371\\272\\332\\004G=\\026\\016\\r\\277Y\\243Q\\275\\201\\2701=\\245\\2579\\276\\351*\\254\\276x\\032\\341\\276e=\\236\\276\\020\\2715\\275\\235\\267\\240\\274\\013\\331\\301=1d\\232>\\300/\\330\\275\\027\\316\\206\\276l\\344\\027\\276\\366\\033\\256\\276\\367\\370\\274>%\\307;\\276E\\306\\231<\\313\\271\\364\\276Vb\\304\\276C\\002\\213<z~@\\276)\\031\\223\\276\\210h\\263\\276f.\\n\\277\\014\\342\\034>\\222L\\371\\2751\\376f\\275Q^9>\\372L\\031\\275\\245\\317\\373;X\\034M>\\303\\233\\370\\276\\250\\3373>~\\202\\257\\276/\\374\\316<\\267\\212\\311\\275\\260\\217/\\276\\252g\\330\\276\\233\\242\\001\\277\\035\\233f=\\243\\250}\\276\\263\\027\\354\\276^\\026\\327\\273\\014\\266\\306\\275dn\\027\\2763\\276\\003\\276c\\335\\375=w\\242\\030>\\365\\263\\325>\\263\\244\\035\\276\\367D0?f\\030\\211\\274>\\265^\\276`\\251W\\276\\375\\210\\312>\\242\\233\\324\\275\\025\\274|\\276<e\\344\\275\\356\\035\\013\\2779\\\\\\004\\276L\\351&\\276\\204\\254Z\\274I\\326\\226\\275\\247*$\\276\\311q\\374=\\366\\327\\025=\\247\\321\\261>\\363\\266\\347\\276\\014\\332\\231>2\\270\\257\\276\\207\\262\\201>j\\316\\214>X\\347\\260>\\001Ug=\\340\\t]\\275a\\034\\023\\275\\357v+\\277\\252d\\231\\276\\350,\\023\\274\\240\\002\\362=\\345\\266\\006=\\227#m\\276\\371?\\271<\\035\\363\\376=\\352\\316J>\\265\\320\\301\\276\\002\\313\\251>\\307\\300\\324\\276h8`\\275\\323x\\206\\275\\342\\225\\264>oX\\215\\276\\301\\274\\317\\276\\213)\\217\\275L\\234\\330\\276;\\201\\304\\275\\033\\000\\003\\275\\027!t<\\211`\\'=\\270\\202\\211=\\3335\\373\\274y\\277\\247\\275Y\\311^\\276\\222\\211\\017\\277\\'j\\232>\\020\\204\\346\\275\\322\\200\\023>\\362\\202\\300\\276,Ey\\275kZ5\\276\\316\\265\\257\\276\\331\\'3\\276\\202&\\254\\2763W\\243\\276\\352\\221\\326=\\277W\\336\\275`\\337\\207\\275rV\\363=\\205`\\313\\273\\312\\021\\263<i\\372A\\274c)\\321\\276\\371\\023\\217>\\204m!\\277`\\237%>t\\211\\234=\\226}v>\\275Q\\227\\276\\247\\206\\310\\276fcy<X\\023\\320\\276\\226k]\\276Q\\332\\201\\276<da\\275\\261\\343\\257\\275;#\\247\\275\\245\\214\\021>\\202\\304\\200\\275\\023Y\\202\\273m_\\274\\276\\261\\250\\363>\\226\\034\\253\\276P\\242\\371=\\256\\251^\\275\\311 4\\276MU\\227\\276]]\\342\\276\\314\\307\\370\\275\\333:r\\276\\373\\035\\273\\276\\346\\246i\\274T\\236\\014=n\\373\\352=\\267\\304V>o\\301\\342\\2755\\201I\\276\\356\\274l>Z\\312/\\277\\250v\\214>\\\\\\277\\320<)\\362&\\276\\266\\n\\213\\276I\\177\\272>\\347\\254\\207\\276 &\\322\\276\\302\\256\\365\\275\\251\\303\\305\\276\\243k\\375\\274xO\\026\\274J\\354\\237=\\330\\022e<W\\2451\\276i\\256\\312<\\320\\227\\306=\"\n  }\n  input {\n    name: \"input.1\"\n    type {\n      tensor_type {\n        elem_type: 1\n        shape {\n          dim {\n            dim_value: 1\n          }\n          dim {\n            dim_value: 9\n          }\n        }\n      }\n    }\n  }\n  output {\n    name: \"12\"\n    type {\n      tensor_type {\n        elem_type: 1\n        shape {\n          dim {\n            dim_value: 1\n          }\n          dim {\n            dim_value: 150\n          }\n        }\n      }\n    }\n  }\n}\n, (tensor([[ 0.2144,  1.4291, -1.2174, -0.6024,  0.2654,  1.4626,  1.5191, -0.3682,\n          1.2702]]),), <function _create_interpreter_name_lookup_fn.<locals>._get_interpreter_name_for_var at 0x7f17e83d9af0>, True, False"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"model.onnx\")\n",
    "\n",
    "# Convert the model to Torch Script\n",
    "script_module = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "# Save the Torch Script module\n",
    "script_module.save(\"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7757706.153 -363881.716      -2.482       8.499 7757721.272 -363870.038\n",
      "       -2.487       4.803       0.001]]\n",
      "[[ 1.16240348 -0.45512402 -1.25481378  0.4579256   1.19118628 -0.4031037\n",
      "  -1.25742504 -1.43256721  0.07519625]]\n",
      "[7757706.153 -363881.716      -2.482       8.499 7757721.272 -363870.038\n",
      "      -2.487       4.803       0.001]\n"
     ]
    }
   ],
   "source": [
    "X_sample2 = X_test_raw[0: 1]\n",
    "print(X_sample2)\n",
    "\n",
    "X_sample2 = scaler.transform(X_sample2)\n",
    "print(X_sample2)\n",
    "X_sample2 = X_test_raw[0]\n",
    "print(X_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7757706.153 -363881.716      -2.482       8.499 7757721.272 -363870.038\n",
      "      -2.487       4.803       0.001] -> [ 4.8010063  -0.00074159  0.01997822  4.8005514  -0.00091523  0.02000268\n",
      "  4.814805   -0.00176182  0.01970529  4.8287263  -0.000778    0.02051836\n",
      "  4.8326116  -0.00174367  0.01992373  4.856453   -0.00167146  0.01983681\n",
      "  4.864167   -0.00193731  0.01993301  4.879089   -0.00164818  0.02014743\n",
      "  4.8937235  -0.00073487  0.02012613  4.8967085  -0.00159477  0.01993886\n",
      "  4.9280586  -0.00181691  0.01975316  4.925619   -0.00195716  0.02010386\n",
      "  4.950785   -0.00190242  0.02001777  4.957052   -0.00186971  0.0202428\n",
      "  4.97818    -0.00249793  0.01988537  4.995721   -0.00255796  0.02014657\n",
      "  4.994455   -0.0025937   0.01995826  5.008133   -0.00280979  0.02011759\n",
      "  5.0308924  -0.00291541  0.01984772  5.0377316  -0.00323275  0.02050596\n",
      "  5.0456805  -0.00307235  0.0199565   5.065904   -0.00340613  0.01975301\n",
      "  5.074639   -0.00332363  0.01996499  5.1010294  -0.00328155  0.01999776\n",
      "  5.1006637  -0.00362112  0.01982425  5.1268015  -0.00374103  0.01991623\n",
      "  5.138431   -0.00395512  0.01991449  5.14359    -0.00391159  0.01998987\n",
      "  5.1597624  -0.00423715  0.0200338   5.164531   -0.00380588  0.01998207\n",
      "  5.1826973  -0.00437438  0.02011609  5.1961102  -0.00469906  0.02033865\n",
      "  5.2155013  -0.00415806  0.02007735  5.2181277  -0.00392795  0.01971572\n",
      "  5.235365   -0.0039203   0.02010624  5.2443867  -0.00465819  0.01985199\n",
      "  5.254088   -0.00413277  0.02006014  5.2746816  -0.0038663   0.0199084\n",
      "  5.2876625  -0.00470388  0.0199628   5.2932086  -0.00375362  0.02029131\n",
      "  5.3144155  -0.0036635   0.02001484  5.3334975  -0.00344844  0.01998478\n",
      "  5.3484445  -0.00428118  0.01996177  5.3510637  -0.00424473  0.0199277\n",
      "  5.3572283  -0.00406776  0.02008447  5.3868194  -0.00447915  0.02004181\n",
      "  5.3942337  -0.00368278  0.01986287  5.4088097  -0.00456195  0.02005492\n",
      "  5.414704   -0.00376444  0.02006499  5.440771   -0.0048934   0.01993128] (expected [4.803 0.001 0.02  4.819 0.001 0.02  4.835 0.001 0.02  4.851 0.001 0.02\n",
      " 4.867 0.001 0.02  4.883 0.001 0.02  4.899 0.001 0.02  4.915 0.001 0.02\n",
      " 4.931 0.001 0.02  4.947 0.001 0.02  4.963 0.001 0.02  4.979 0.001 0.02\n",
      " 4.995 0.001 0.02  5.011 0.001 0.02  5.027 0.001 0.02  5.043 0.001 0.02\n",
      " 5.059 0.001 0.02  5.075 0.001 0.02  5.091 0.001 0.02  5.107 0.001 0.02\n",
      " 5.123 0.001 0.02  5.139 0.001 0.02  5.155 0.001 0.02  5.171 0.    0.02\n",
      " 5.187 0.    0.02  5.203 0.    0.02  5.219 0.    0.02  5.235 0.    0.02\n",
      " 5.251 0.    0.02  5.267 0.    0.02  5.283 0.    0.02  5.299 0.    0.02\n",
      " 5.315 0.    0.02  5.331 0.    0.02  5.347 0.    0.02  5.363 0.    0.02\n",
      " 5.379 0.    0.02  5.395 0.    0.02  5.411 0.    0.02  5.427 0.    0.02\n",
      " 5.443 0.    0.02  5.459 0.    0.02  5.475 0.    0.02  5.491 0.    0.02\n",
      " 5.507 0.    0.02  5.523 0.    0.02  5.539 0.    0.02  5.555 0.    0.02\n",
      " 5.571 0.    0.02  5.587 0.001 0.02 ])\n",
      "[7757801.761 -363519.424       3.074       8.016 7757831.521 -363531.805\n",
      "       2.493       8.334       0.025] -> [8.332953   0.02570583 0.02057826 8.279287   0.0260936  0.01937635\n",
      " 8.279661   0.02568588 0.02047574 8.281424   0.02454777 0.01915007\n",
      " 8.274772   0.02515006 0.02098195 8.281982   0.02503929 0.01914372\n",
      " 8.278026   0.02583499 0.0184833  8.280119   0.02489562 0.02037758\n",
      " 8.2803     0.02479535 0.02028337 8.273154   0.02475645 0.01960913\n",
      " 8.286489   0.02491888 0.01928371 8.275139   0.02615823 0.01904969\n",
      " 8.283588   0.0249912  0.020569   8.279796   0.02533591 0.02059739\n",
      " 8.283659   0.02476165 0.02070565 8.287676   0.02572682 0.02014912\n",
      " 8.276308   0.02517223 0.01923848 8.27715    0.02432403 0.01998729\n",
      " 8.284091   0.0255501  0.01925227 8.280431   0.02544269 0.02009993\n",
      " 8.276081   0.0244431  0.01982384 8.280431   0.02421162 0.0209767\n",
      " 8.276962   0.02461271 0.02015173 8.288047   0.02541332 0.02063011\n",
      " 8.277835   0.02415167 0.02029199 8.287139   0.0238065  0.01952587\n",
      " 8.285027   0.02313591 0.01905958 8.280449   0.02377018 0.01999696\n",
      " 8.28143    0.02516142 0.02103802 8.275567   0.02468783 0.02011264\n",
      " 8.279786   0.02446736 0.0203737  8.279707   0.02499932 0.01933622\n",
      " 8.283042   0.02523439 0.01940995 8.276601   0.02493435 0.01961403\n",
      " 8.27914    0.02568574 0.02154677 8.276657   0.02521071 0.02053792\n",
      " 8.274283   0.02505938 0.02104003 8.278971   0.02595724 0.01849017\n",
      " 8.2789345  0.02633464 0.01930214 8.273977   0.02619009 0.02057384\n",
      " 8.278184   0.02495228 0.0193009  8.282623   0.02417462 0.0207606\n",
      " 8.285052   0.02478037 0.02051708 8.276708   0.02573486 0.01951893\n",
      " 8.273311   0.02499515 0.0200881  8.283986   0.02586628 0.01974319\n",
      " 8.279172   0.02529712 0.02044044 8.281159   0.02531607 0.01932614\n",
      " 8.275503   0.02524789 0.01950336 8.284525   0.02565493 0.01986678] (expected [8.334 0.025 0.02  8.332 0.025 0.02  8.331 0.025 0.02  8.329 0.025 0.02\n",
      " 8.327 0.025 0.02  8.326 0.025 0.02  8.324 0.025 0.02  8.323 0.025 0.02\n",
      " 8.321 0.025 0.02  8.319 0.025 0.02  8.318 0.025 0.02  8.316 0.025 0.02\n",
      " 8.315 0.025 0.02  8.313 0.025 0.02  8.312 0.025 0.02  8.31  0.025 0.02\n",
      " 8.308 0.026 0.02  8.307 0.026 0.02  8.305 0.026 0.02  8.304 0.026 0.02\n",
      " 8.302 0.026 0.02  8.3   0.026 0.02  8.299 0.026 0.02  8.297 0.026 0.02\n",
      " 8.296 0.026 0.02  8.294 0.026 0.02  8.292 0.026 0.02  8.291 0.026 0.02\n",
      " 8.289 0.027 0.02  8.288 0.027 0.02  8.286 0.027 0.02  8.285 0.027 0.02\n",
      " 8.283 0.027 0.02  8.281 0.027 0.02  8.28  0.027 0.02  8.278 0.028 0.02\n",
      " 8.277 0.028 0.02  8.275 0.028 0.02  8.273 0.028 0.02  8.272 0.029 0.02\n",
      " 8.27  0.029 0.02  8.269 0.029 0.02  8.267 0.029 0.02  8.265 0.03  0.02\n",
      " 8.264 0.03  0.02  8.262 0.03  0.02  8.261 0.031 0.02  8.259 0.031 0.02\n",
      " 8.257 0.031 0.02  8.256 0.032 0.02 ])\n",
      "[7757398.832 -363523.465       2.313       8.55  7757418.023 -363547.615\n",
      "       2.224       7.703       0.001] -> [7.68352    0.00130333 0.02032702 7.641883   0.00083899 0.02039786\n",
      " 7.6465163  0.00056658 0.02013385 7.6483893  0.00194509 0.02029281\n",
      " 7.6505     0.00100994 0.01988105 7.6568565  0.00122235 0.02029719\n",
      " 7.6596446  0.00039227 0.02037289 7.662617   0.00142402 0.02003271\n",
      " 7.667677   0.00241441 0.01962855 7.669256   0.00201954 0.02052943\n",
      " 7.6761103  0.00163754 0.01999038 7.6771     0.00058164 0.02000098\n",
      " 7.683269   0.00165032 0.02034155 7.684547   0.00150722 0.01954996\n",
      " 7.6919785  0.00146589 0.02018756 7.695269   0.0005441  0.0205392\n",
      " 7.6975493  0.00094914 0.02031845 7.700929   0.00180373 0.02080054\n",
      " 7.7061877  0.00045523 0.02029929 7.7079177  0.00016031 0.01984137\n",
      " 7.711619   0.00095294 0.01972057 7.7176385  0.00099961 0.01966164\n",
      " 7.720926   0.00110845 0.02040421 7.725027   0.00096409 0.01933991\n",
      " 7.726849   0.00044702 0.01945675 7.7329917  0.00127519 0.02017855\n",
      " 7.737667   0.00104059 0.02027552 7.738297   0.00138625 0.02069708\n",
      " 7.7441926  0.00089028 0.02004447 7.7461205  0.00130421 0.02008313\n",
      " 7.749738   0.00169409 0.02011915 7.7536077  0.00105089 0.02062368\n",
      " 7.760129   0.00061737 0.02020869 7.760017   0.0009951  0.01945942\n",
      " 7.7652626  0.00119765 0.01941936 7.7676105  0.00134489 0.01985163\n",
      " 7.770476   0.00189952 0.01999766 7.776159   0.00147761 0.02066216\n",
      " 7.7798986  0.00109136 0.01993258 7.7812815  0.0016046  0.02023147\n",
      " 7.788701   0.00245623 0.02079733 7.793334   0.00313549 0.02034777\n",
      " 7.795774   0.00259195 0.01947185 7.7981896  0.00216164 0.01940961\n",
      " 7.7987413  0.0028218  0.01963487 7.8069115  0.00203289 0.02025949\n",
      " 7.810602   0.00299086 0.02001701 7.8133383  0.00247199 0.01974632\n",
      " 7.816203   0.00330944 0.02070255 7.8224535  0.00220504 0.01950284] (expected [7.703 0.001 0.02  7.707 0.001 0.02  7.712 0.001 0.02  7.716 0.001 0.02\n",
      " 7.72  0.001 0.02  7.725 0.001 0.02  7.729 0.001 0.02  7.734 0.001 0.02\n",
      " 7.738 0.001 0.02  7.742 0.001 0.02  7.747 0.001 0.02  7.751 0.001 0.02\n",
      " 7.755 0.001 0.02  7.76  0.001 0.02  7.764 0.001 0.02  7.769 0.001 0.02\n",
      " 7.773 0.001 0.02  7.777 0.001 0.02  7.782 0.001 0.02  7.786 0.001 0.02\n",
      " 7.791 0.001 0.02  7.795 0.001 0.02  7.799 0.001 0.02  7.804 0.001 0.02\n",
      " 7.808 0.001 0.02  7.813 0.001 0.02  7.817 0.001 0.02  7.821 0.001 0.02\n",
      " 7.826 0.001 0.02  7.83  0.001 0.02  7.835 0.001 0.02  7.839 0.001 0.02\n",
      " 7.843 0.001 0.02  7.848 0.001 0.02  7.852 0.001 0.02  7.857 0.001 0.02\n",
      " 7.861 0.001 0.02  7.865 0.001 0.02  7.87  0.001 0.02  7.874 0.001 0.02\n",
      " 7.878 0.001 0.02  7.883 0.001 0.02  7.887 0.001 0.02  7.892 0.001 0.02\n",
      " 7.896 0.001 0.02  7.9   0.001 0.02  7.905 0.001 0.02  7.909 0.001 0.02\n",
      " 7.914 0.001 0.02  7.918 0.001 0.02 ])\n",
      "[7756536.421 -363779.861       1.18        8.55  7756528.942 -363813.723\n",
      "       1.441       8.68       -0.007] -> [ 8.648965   -0.00748847  0.02032312  8.593876   -0.00899696  0.02150925\n",
      "  8.5948105  -0.00896706  0.02008479  8.58801    -0.00458245  0.02139455\n",
      "  8.591499   -0.00721812  0.01868896  8.588325   -0.00662043  0.02156892\n",
      "  8.590234   -0.00932901  0.02237298  8.5854225  -0.00596195  0.0196064\n",
      "  8.587645   -0.00375348  0.01870826  8.590537   -0.00439189  0.02173356\n",
      "  8.581299   -0.00550023  0.02065581  8.587846   -0.00899826  0.02101593\n",
      "  8.5828     -0.00535823  0.02023461  8.580964   -0.0060997   0.01808261\n",
      "  8.584315   -0.00537994  0.01971883  8.577991   -0.00857803  0.02118644\n",
      "  8.587483   -0.00700641  0.021599    8.58501    -0.00396815  0.02187604\n",
      "  8.579759   -0.00841048  0.02152368  8.5788145  -0.00893942  0.01926094\n",
      "  8.583215   -0.00601663  0.01948993  8.582711   -0.00559088  0.01828561\n",
      "  8.585115   -0.00567044  0.02085053  8.572363   -0.00679334  0.01776277\n",
      "  8.579396   -0.00675549  0.0184348   8.573252   -0.00431146  0.02105259\n",
      "  8.577254   -0.00418436  0.02162115  8.574989   -0.00399152  0.0217767\n",
      "  8.57805    -0.00653502  0.01914867  8.58017    -0.00506145  0.02012574\n",
      "  8.574344   -0.00387561  0.01986536  8.574201   -0.00594503  0.02222824\n",
      "  8.575941   -0.00736262  0.02103469  8.574182   -0.00618547  0.0190888\n",
      "  8.573944   -0.00645809  0.01701276  8.573064   -0.00555292  0.0190782\n",
      "  8.573169   -0.00409814  0.01893005  8.571572   -0.00615226  0.0231559\n",
      "  8.570943   -0.00733435  0.02050165  8.570824   -0.00616522  0.01997779\n",
      "  8.573595   -0.00285508  0.02266856  8.569768   -0.0005482   0.0200972\n",
      "  8.563346   -0.00242199  0.01813856  8.569615   -0.00433783  0.01900239\n",
      "  8.565632   -0.00200707  0.01899615  8.562543   -0.00488578  0.02088645\n",
      "  8.567423   -0.00223367  0.01964829  8.562147   -0.00337069  0.01994379\n",
      "  8.566233   -0.00156729  0.02223463  8.560793   -0.00444076  0.01894637] (expected [ 8.68  -0.007  0.02   8.55  -0.007  0.02   8.55  -0.007  0.02   8.55\n",
      " -0.007  0.02   8.55  -0.007  0.02   8.55  -0.007  0.02   8.55  -0.007\n",
      "  0.02   8.55  -0.007  0.02   8.55  -0.007  0.02   8.55  -0.007  0.02\n",
      "  8.55  -0.007  0.02   8.55  -0.007  0.02   8.55  -0.007  0.02   8.55\n",
      " -0.007  0.02   8.55  -0.007  0.02   8.55  -0.007  0.02   8.55  -0.007\n",
      "  0.02   8.55  -0.007  0.02   8.55  -0.007  0.02   8.55  -0.007  0.02\n",
      "  8.55  -0.007  0.02   8.55  -0.008  0.02   8.55  -0.008  0.02   8.55\n",
      " -0.008  0.02   8.55  -0.008  0.02   8.55  -0.008  0.02   8.55  -0.008\n",
      "  0.02   8.55  -0.008  0.02   8.55  -0.008  0.02   8.55  -0.008  0.02\n",
      "  8.55  -0.008  0.02   8.55  -0.008  0.02   8.55  -0.008  0.02   8.55\n",
      " -0.008  0.02   8.55  -0.008  0.02   8.55  -0.008  0.02   8.55  -0.008\n",
      "  0.02   8.55  -0.008  0.02   8.55  -0.008  0.02   8.55  -0.009  0.02\n",
      "  8.55  -0.009  0.02   8.55  -0.009  0.02   8.55  -0.009  0.02   8.55\n",
      " -0.009  0.02   8.55  -0.009  0.02   8.55  -0.009  0.02   8.55  -0.009\n",
      "  0.02   8.55  -0.009  0.02   8.55  -0.009  0.02   8.55  -0.009  0.02 ])\n",
      "[7757309.348 -363499.087      -2.407       6.407 7757333.518 -363490.282\n",
      "       3.096       6.789       0.063] -> [6.7516246  0.06393541 0.0203922  6.714112   0.06429583 0.01986296\n",
      " 6.7130346  0.06421862 0.02037632 6.714601   0.06174846 0.01862007\n",
      " 6.690399   0.06312048 0.02134422 6.7101917  0.06299231 0.01969864\n",
      " 6.697115   0.0635093  0.01890697 6.6980944  0.06255621 0.02043486\n",
      " 6.696756   0.06170458 0.02021852 6.673561   0.0622045  0.01976297\n",
      " 6.710045   0.06274613 0.02008623 6.674609   0.0626265  0.01846962\n",
      " 6.6977534  0.06200032 0.02084127 6.681123   0.06232089 0.02072971\n",
      " 6.691144   0.06180153 0.02126975 6.7009463  0.06254116 0.01991961\n",
      " 6.6646037  0.06192434 0.01919109 6.663426   0.06095931 0.02039833\n",
      " 6.6841207  0.06183478 0.01955387 6.6679854  0.06177959 0.01980429\n",
      " 6.652021   0.06063057 0.01983571 6.6637483  0.06052557 0.02118478\n",
      " 6.650559   0.06046109 0.02037393 6.679942   0.06084244 0.02039348\n",
      " 6.64709    0.05994342 0.02032236 6.6720653  0.05926207 0.01921879\n",
      " 6.664455   0.05888236 0.0193051  6.647466   0.0593219  0.0200149\n",
      " 6.6470604  0.06064299 0.02071782 6.626414   0.05951655 0.02003768\n",
      " 6.635787   0.05980266 0.02034537 6.635339   0.06039565 0.01852893\n",
      " 6.6426735  0.06026457 0.01953453 6.6192966  0.05949992 0.02007317\n",
      " 6.6249437  0.06008597 0.02125828 6.615039   0.05911204 0.02098817\n",
      " 6.6027455  0.05909542 0.02104271 6.616658   0.06002344 0.01874468\n",
      " 6.613754   0.06030071 0.01947895 6.5958104  0.05995737 0.01991723\n",
      " 6.606965   0.05849443 0.01943179 6.617211   0.05763273 0.02095491\n",
      " 6.620795   0.05855547 0.02065229 6.593711   0.05880125 0.01958214\n",
      " 6.578475   0.05770756 0.01979366 6.6092935  0.05872704 0.01973746\n",
      " 6.593334   0.05818869 0.02065203 6.594169   0.05764527 0.0194798\n",
      " 6.5751987  0.05768259 0.01949466 6.601967   0.05686679 0.01971253] (expected [6.789 0.063 0.02  6.787 0.063 0.02  6.785 0.063 0.02  6.783 0.063 0.02\n",
      " 6.781 0.063 0.02  6.779 0.063 0.02  6.777 0.063 0.02  6.775 0.063 0.02\n",
      " 6.773 0.063 0.02  6.771 0.063 0.02  6.769 0.063 0.02  6.767 0.063 0.02\n",
      " 6.765 0.063 0.02  6.763 0.063 0.02  6.761 0.063 0.02  6.759 0.063 0.02\n",
      " 6.757 0.064 0.02  6.755 0.064 0.02  6.753 0.064 0.02  6.751 0.064 0.02\n",
      " 6.75  0.064 0.02  6.748 0.064 0.02  6.746 0.064 0.02  6.744 0.064 0.02\n",
      " 6.742 0.064 0.02  6.74  0.064 0.02  6.738 0.064 0.02  6.736 0.065 0.02\n",
      " 6.734 0.065 0.02  6.732 0.065 0.02  6.73  0.065 0.02  6.728 0.065 0.02\n",
      " 6.726 0.066 0.02  6.724 0.066 0.02  6.722 0.066 0.02  6.72  0.066 0.02\n",
      " 6.718 0.067 0.02  6.716 0.067 0.02  6.714 0.067 0.02  6.712 0.067 0.02\n",
      " 6.71  0.068 0.02  6.708 0.068 0.02  6.706 0.068 0.02  6.705 0.069 0.02\n",
      " 6.703 0.069 0.02  6.701 0.07  0.02  6.699 0.07  0.02  6.697 0.071 0.02\n",
      " 6.695 0.071 0.02  6.693 0.072 0.02 ])\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "np.set_printoptions(suppress=True)\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.00\n",
      "RMSE: 0.02\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.75696163e+06 -3.64057635e+05 -1.38000000e-01  8.54600000e+00\n",
      "  7.75692823e+06 -3.64053126e+05 -1.32000000e-01  8.49100000e+00\n",
      " -2.00000000e-03] -> [ 8.4910440e+00 -2.1275878e-03  2.0009704e-02] (expected [ 8.491e+00 -2.000e-03  2.000e-02])\n",
      "[ 7.75720516e+06 -3.63589039e+05 -2.53100000e+00  8.55000000e+00\n",
      "  7.75723371e+06 -3.63569516e+05 -2.49900000e+00  8.66100000e+00\n",
      " -6.00000000e-03] -> [ 8.6609583e+00 -6.1482787e-03  2.0036645e-02] (expected [ 8.661e+00 -6.000e-03  2.000e-02])\n",
      "[ 7.75717733e+06 -3.63615552e+05  1.04800000e+00  4.16700000e+00\n",
      "  7.75716855e+06 -3.63636256e+05  1.09200000e+00  5.54400000e+00\n",
      "  6.30000000e-02] -> [5.5442753  0.06288677 0.02018113] (expected [5.544 0.063 0.02 ])\n",
      "[ 7.75719467e+06 -3.63595825e+05 -2.57200000e+00  8.52500000e+00\n",
      "  7.75722353e+06 -3.63576869e+05 -2.54100000e+00  8.66100000e+00\n",
      " -6.00000000e-03] -> [ 8.6609735e+00 -6.1470866e-03  2.0041056e-02] (expected [ 8.661e+00 -6.000e-03  2.000e-02])\n",
      "[ 7.75663871e+06 -3.63686725e+05 -2.46600000e+00  8.55000000e+00\n",
      "  7.75666680e+06 -3.63666710e+05 -2.61800000e+00  8.69300000e+00\n",
      "  2.20000000e-02] -> [8.693061   0.02189618 0.01997633] (expected [8.693 0.022 0.02 ])\n"
     ]
    }
   ],
   "source": [
    "#FUNCIONOU COM 3 SAIDAS\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# Read data\n",
    "#data = fetch_california_housing()\n",
    "X = np.loadtxt(dataset_input,dtype='float',delimiter=\";\",usecols=np.arange(0,9))\n",
    "y = np.loadtxt(dataset_output,dtype='float',delimiter=\";\",usecols=np.arange(0,3))\n",
    "#X, y = data.data, data.target\n",
    " \n",
    "# train-test split for model evaluation\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    " \n",
    "\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    " \n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 3)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 3)\n",
    " \n",
    "# Define the model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 24),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(24, 12),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(12, 6),\n",
    "    #torch.nn.ReLU(),\n",
    "    torch.nn.Linear(6, 3)\n",
    ")\n",
    " \n",
    "# loss function and optimizer\n",
    "loss_fn = torch.nn.MSELoss()  # mean square error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 8  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            end = min(start+batch_size, len(X_train))  # Add this line\n",
    "            X_batch = X_train[start:end]  # Modify this line\n",
    "            y_batch = y_train[start:end]  # Modify this line\n",
    "            #X_batch = X_train[start:start+batch_size]\n",
    "            #y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    " \n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    " \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.00\n",
      "RMSE: 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.75655038e+06 -3.63917271e+05 -2.31000000e-01  5.70200000e+00\n",
      "  7.75653753e+06 -3.63908823e+05 -8.46000000e-01  4.40600000e+00\n",
      "  7.50000000e-02] -> [4.3999896] (expected [4.406])\n",
      "[ 7.75766160e+06 -3.63610685e+05 -2.50900000e+00  8.55000000e+00\n",
      "  7.75768651e+06 -3.63592380e+05 -2.50700000e+00  7.70300000e+00\n",
      " -2.00000000e-03] -> [7.704249] (expected [7.703])\n",
      "[ 7.75754790e+06 -3.63656925e+05 -3.13600000e+00  8.55000000e+00\n",
      "  7.75757851e+06 -3.63654357e+05 -2.96700000e+00  7.70300000e+00\n",
      " -1.50000000e-02] -> [7.705766] (expected [7.703])\n",
      "[ 7.75769361e+06 -3.63590014e+05  6.39000000e-01  8.55000000e+00\n",
      "  7.75766713e+06 -3.63610291e+05  6.52000000e-01  8.52900000e+00\n",
      "  3.00000000e-03] -> [8.52703] (expected [8.529])\n",
      "[ 7.75685868e+06 -3.64044085e+05 -1.26000000e-01  8.55000000e+00\n",
      "  7.75682885e+06 -3.64038885e+05 -1.75000000e-01  7.65400000e+00\n",
      "  1.00000000e-03] -> [7.647367] (expected [7.654])\n"
     ]
    }
   ],
   "source": [
    "#FUNCIONOU (uma saida)\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# Read data\n",
    "#data = fetch_california_housing()\n",
    "X = np.loadtxt(dataset_input,dtype='float',delimiter=\";\",usecols=np.arange(0,9))\n",
    "y = np.loadtxt(dataset_output,dtype='float',delimiter=\";\",usecols=np.arange(0,1))\n",
    "#X, y = data.data, data.target\n",
    " \n",
    "# train-test split for model evaluation\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    " \n",
    "\n",
    "\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test = scaler.transform(X_test_raw)\n",
    " \n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    " \n",
    "# Define the model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(9, 24),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(24, 12),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(12, 6),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(6, 1)\n",
    ")\n",
    " \n",
    "# loss function and optimizer\n",
    "loss_fn = torch.nn.MSELoss()  # mean square error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    " \n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 8  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    " \n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    " \n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()\n",
    " \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        X_sample = X_test_raw[i: i+1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_pred = model(X_sample)\n",
    "        print(f\"{X_test_raw[i]} -> {y_pred[0].numpy()} (expected {y_test[i].numpy()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Criando o modelo \n",
    "l0 = tf.keras.layers.Dense(units = 9, input_shape = [9])\n",
    "l1 = tf.keras.layers.Dense(units = 64)\n",
    "l2 = tf.keras.layers.Dense(units = 64)\n",
    "#l3 = tf.keras.layers.Dense(units = 93)\n",
    "l3 = tf.keras.layers.Dense(units = 3)\n",
    "\n",
    "\"\"\"Modelo inicial: \n",
    "l0 = tf.keras.layers.Dense(units = 4, input_shape = [4])\n",
    "l1 = tf.keras.layers.Dense(units = 64)\n",
    "l2 = tf.keras.layers.Dense(units = 128)\n",
    "l3 = tf.keras.layers.Dense(units = 3) \"\"\"\n",
    "\n",
    "model = tf.keras.Sequential([l0,l1,l2,l3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Compilando o modelo\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=1e-4))#tf.keras.optimizers.Adam(0.1)), loss='mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Treinar o modelo\n",
    "history = model.fit(inputMatrix,outputMatrix,epochs=500,verbose=False)#epochs inicial=500\n",
    "print(\"Finished training the model!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
